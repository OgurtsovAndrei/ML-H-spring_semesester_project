{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-lightning in /home/andrey/.local/share/virtualenvs/ML-H-spring_semesester_project-nJO6SWcT/lib/python3.10/site-packages (2.0.6)\r\n",
      "Requirement already satisfied: numpy>=1.17.2 in /home/andrey/.local/share/virtualenvs/ML-H-spring_semesester_project-nJO6SWcT/lib/python3.10/site-packages (from pytorch-lightning) (1.25.2)\r\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/andrey/.local/share/virtualenvs/ML-H-spring_semesester_project-nJO6SWcT/lib/python3.10/site-packages (from pytorch-lightning) (2.0.1)\r\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /home/andrey/.local/share/virtualenvs/ML-H-spring_semesester_project-nJO6SWcT/lib/python3.10/site-packages (from pytorch-lightning) (4.65.0)\r\n",
      "Requirement already satisfied: PyYAML>=5.4 in /home/andrey/.local/share/virtualenvs/ML-H-spring_semesester_project-nJO6SWcT/lib/python3.10/site-packages (from pytorch-lightning) (6.0.1)\r\n",
      "Requirement already satisfied: fsspec[http]>2021.06.0 in /home/andrey/.local/share/virtualenvs/ML-H-spring_semesester_project-nJO6SWcT/lib/python3.10/site-packages (from pytorch-lightning) (2023.6.0)\r\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in /home/andrey/.local/share/virtualenvs/ML-H-spring_semesester_project-nJO6SWcT/lib/python3.10/site-packages (from pytorch-lightning) (1.0.1)\r\n",
      "Requirement already satisfied: packaging>=17.1 in /home/andrey/.local/share/virtualenvs/ML-H-spring_semesester_project-nJO6SWcT/lib/python3.10/site-packages (from pytorch-lightning) (23.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/andrey/.local/share/virtualenvs/ML-H-spring_semesester_project-nJO6SWcT/lib/python3.10/site-packages (from pytorch-lightning) (4.7.1)\r\n",
      "Requirement already satisfied: lightning-utilities>=0.7.0 in /home/andrey/.local/share/virtualenvs/ML-H-spring_semesester_project-nJO6SWcT/lib/python3.10/site-packages (from pytorch-lightning) (0.9.0)\r\n",
      "Requirement already satisfied: requests in /home/andrey/.local/share/virtualenvs/ML-H-spring_semesester_project-nJO6SWcT/lib/python3.10/site-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (2.31.0)\r\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/andrey/.local/share/virtualenvs/ML-H-spring_semesester_project-nJO6SWcT/lib/python3.10/site-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (3.8.5)\r\n",
      "Requirement already satisfied: filelock in /home/andrey/.local/share/virtualenvs/ML-H-spring_semesester_project-nJO6SWcT/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (3.12.2)\r\n",
      "Requirement already satisfied: sympy in /home/andrey/.local/share/virtualenvs/ML-H-spring_semesester_project-nJO6SWcT/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (1.12)\r\n",
      "Requirement already satisfied: networkx in /home/andrey/.local/share/virtualenvs/ML-H-spring_semesester_project-nJO6SWcT/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (3.1)\r\n",
      "Requirement already satisfied: jinja2 in /home/andrey/.local/share/virtualenvs/ML-H-spring_semesester_project-nJO6SWcT/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (3.1.2)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/andrey/.local/share/virtualenvs/ML-H-spring_semesester_project-nJO6SWcT/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (11.7.99)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/andrey/.local/share/virtualenvs/ML-H-spring_semesester_project-nJO6SWcT/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (11.7.99)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/andrey/.local/share/virtualenvs/ML-H-spring_semesester_project-nJO6SWcT/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (11.7.101)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/andrey/.local/share/virtualenvs/ML-H-spring_semesester_project-nJO6SWcT/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (8.5.0.96)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/andrey/.local/share/virtualenvs/ML-H-spring_semesester_project-nJO6SWcT/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (11.10.3.66)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/andrey/.local/share/virtualenvs/ML-H-spring_semesester_project-nJO6SWcT/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (10.9.0.58)\r\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/andrey/.local/share/virtualenvs/ML-H-spring_semesester_project-nJO6SWcT/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (10.2.10.91)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/andrey/.local/share/virtualenvs/ML-H-spring_semesester_project-nJO6SWcT/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (11.4.0.1)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/andrey/.local/share/virtualenvs/ML-H-spring_semesester_project-nJO6SWcT/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (11.7.4.91)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/andrey/.local/share/virtualenvs/ML-H-spring_semesester_project-nJO6SWcT/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (2.14.3)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/andrey/.local/share/virtualenvs/ML-H-spring_semesester_project-nJO6SWcT/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (11.7.91)\r\n",
      "Requirement already satisfied: triton==2.0.0 in /home/andrey/.local/share/virtualenvs/ML-H-spring_semesester_project-nJO6SWcT/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (2.0.0)\r\n",
      "Requirement already satisfied: setuptools in /home/andrey/.local/share/virtualenvs/ML-H-spring_semesester_project-nJO6SWcT/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.11.0->pytorch-lightning) (68.0.0)\r\n",
      "Requirement already satisfied: wheel in /home/andrey/.local/share/virtualenvs/ML-H-spring_semesester_project-nJO6SWcT/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.11.0->pytorch-lightning) (0.41.0)\r\n",
      "Requirement already satisfied: cmake in /home/andrey/.local/share/virtualenvs/ML-H-spring_semesester_project-nJO6SWcT/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.11.0->pytorch-lightning) (3.27.0)\r\n",
      "Requirement already satisfied: lit in /home/andrey/.local/share/virtualenvs/ML-H-spring_semesester_project-nJO6SWcT/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.11.0->pytorch-lightning) (16.0.6)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/andrey/.local/share/virtualenvs/ML-H-spring_semesester_project-nJO6SWcT/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (23.1.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/andrey/.local/share/virtualenvs/ML-H-spring_semesester_project-nJO6SWcT/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (3.2.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/andrey/.local/share/virtualenvs/ML-H-spring_semesester_project-nJO6SWcT/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (6.0.4)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/andrey/.local/share/virtualenvs/ML-H-spring_semesester_project-nJO6SWcT/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (4.0.2)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/andrey/.local/share/virtualenvs/ML-H-spring_semesester_project-nJO6SWcT/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.9.2)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/andrey/.local/share/virtualenvs/ML-H-spring_semesester_project-nJO6SWcT/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.4.0)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/andrey/.local/share/virtualenvs/ML-H-spring_semesester_project-nJO6SWcT/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/andrey/.local/share/virtualenvs/ML-H-spring_semesester_project-nJO6SWcT/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->pytorch-lightning) (2.1.3)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/andrey/.local/share/virtualenvs/ML-H-spring_semesester_project-nJO6SWcT/lib/python3.10/site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/andrey/.local/share/virtualenvs/ML-H-spring_semesester_project-nJO6SWcT/lib/python3.10/site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2.0.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/andrey/.local/share/virtualenvs/ML-H-spring_semesester_project-nJO6SWcT/lib/python3.10/site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2023.7.22)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/andrey/.local/share/virtualenvs/ML-H-spring_semesester_project-nJO6SWcT/lib/python3.10/site-packages (from sympy->torch>=1.11.0->pytorch-lightning) (1.3.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-lightning"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-08-04T09:34:26.225133798Z",
     "start_time": "2023-08-04T09:34:23.544681565Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-08-04T09:34:31.895730097Z",
     "start_time": "2023-08-04T09:34:26.228399812Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from src.tools import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-08-04T09:34:31.898343106Z",
     "start_time": "2023-08-04T09:34:31.892338304Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=3, edgeitems=20, linewidth=250)   "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T09:34:31.910867126Z",
     "start_time": "2023-08-04T09:34:31.897488820Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class SamenessModule(nn.Module):\n",
    "    def __init__(self, layer_sizes=(1, 8, 8, 8, 1), activation_fun=nn.ReLU()):\n",
    "        super().__init__()\n",
    "\n",
    "        self.run_counter = 0\n",
    "\n",
    "        layers_list = []\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            layers_list.append(nn.Linear(layer_sizes[i], layer_sizes[i + 1]))\n",
    "            if i != len(layer_sizes) - 2:\n",
    "                layers_list.append(activation_fun)\n",
    "\n",
    "        self.l1 = nn.Sequential(*layers_list)\n",
    "        print(self)\n",
    "\n",
    "        for layer in self.l1:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                layer.weight = nn.Parameter(layer.weight.double())\n",
    "                layer.bias = nn.Parameter(layer.bias.double())\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.run_counter += 1\n",
    "        return self.l1(x.double())\n",
    "\n",
    "\n",
    "class SamenessAutoEncoder(pl.LightningModule):\n",
    "    def __init__(self, encoder: SamenessModule):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        batch = batch[:, 0, :]\n",
    "        x, y = batch.reshape(2, len(batch), 1)\n",
    "        x_hat = self.encoder(x)\n",
    "        loss = F.mse_loss(x_hat, x)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=5 * 1e-3)\n",
    "        return optimizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-08-04T09:34:31.913872391Z",
     "start_time": "2023-08-04T09:34:31.904949981Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class SamenessBinModule(nn.Module):\n",
    "    def __init__(self, layer_sizes=(64, 8, 8, 8, 64), activation_fun=nn.ReLU()):\n",
    "        super().__init__()\n",
    "\n",
    "        self.run_counter = 0\n",
    "\n",
    "        layers_list = []\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            layers_list.append(nn.Linear(layer_sizes[i], layer_sizes[i + 1]))\n",
    "            if i != len(layer_sizes) - 2:\n",
    "                layers_list.append(activation_fun)\n",
    "\n",
    "        self.l1 = nn.Sequential(*layers_list)\n",
    "        print(self)\n",
    "\n",
    "        for layer in self.l1:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                layer.weight = nn.Parameter(layer.weight.double())\n",
    "                layer.bias = nn.Parameter(layer.bias.double())\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.run_counter += 1\n",
    "        return self.l1(x.double())\n",
    "\n",
    "\n",
    "class SamenessBinAutoEncoder(pl.LightningModule):\n",
    "    def __init__(self, encoder: SamenessModule):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        batch = batch[:, 0, :]\n",
    "        x, y = batch.reshape(2, len(batch), 64)\n",
    "        x_hat = self.encoder(x)\n",
    "        loss = F.mse_loss(x_hat, x)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=5 * 1e-3)\n",
    "        return optimizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-08-04T09:34:31.982706270Z",
     "start_time": "2023-08-04T09:34:31.918405397Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import os\n",
    "import torch.random\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-08-04T09:34:32.549425324Z",
     "start_time": "2023-08-04T09:34:31.925271540Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from notebooks.data.SimpleRandomDataset import SimpleRandomDataset\n",
    "from notebooks.data.SimpleRandomDataset import SimpleRandomBitDataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-08-04T09:34:32.582995483Z",
     "start_time": "2023-08-04T09:34:32.550911501Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Parameters:\n",
    "activation_fun = nn.ReLU()\n",
    "layer_sizes = [1, 8, 8, 8, 1]\n",
    "# layer_sizes = [1, 32, 32, 32, 1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-08-04T09:34:32.595084450Z",
     "start_time": "2023-08-04T09:34:32.561750300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Training with:\n",
    "n = 50000 batch_size = 5000 max_epochs=100 -> takes 102 sec\n",
    "n = 50000 batch_size = 2500 max_epochs=100 -> takes 66 sec\n",
    "n = 50000 batch_size = 1000 max_epochs=100 -> takes 81 sec\n",
    "n = 50000 batch_size = 500 max_epochs=100 -> takes 107 sec"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# dataset = SimpleRandomDataset(50000, transform=transforms.ToTensor())\n",
    "# train_loader = DataLoader(dataset, batch_size=2500,\n",
    "#                           # num_workers=8\n",
    "#                           )\n",
    "#\n",
    "# # model\n",
    "# autoencoder = SamenessAutoEncoder(SamenessModule(layer_sizes, activation_fun))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-08-04T09:34:32.700363326Z",
     "start_time": "2023-08-04T09:34:32.567848321Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SamenessBinModule(\n",
      "  (l1): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=8, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=8, out_features=8, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=8, out_features=8, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=8, out_features=64, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "dataset = SimpleRandomBitDataset(50000, transform=transforms.ToTensor())\n",
    "train_loader = DataLoader(dataset, batch_size=2500,\n",
    "                          # num_workers=8\n",
    "                          )\n",
    "\n",
    "# model\n",
    "autoencoder = SamenessBinAutoEncoder(SamenessBinModule())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-08-04T09:34:33.574330935Z",
     "start_time": "2023-08-04T09:34:32.574968952Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrey/.local/share/virtualenvs/ML-H-spring_semesester_project-nJO6SWcT/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3050 Ti Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type              | Params\n",
      "----------------------------------------------\n",
      "0 | encoder | SamenessBinModule | 1.2 K \n",
      "----------------------------------------------\n",
      "1.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n",
      "/home/andrey/.local/share/virtualenvs/ML-H-spring_semesester_project-nJO6SWcT/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/andrey/.local/share/virtualenvs/ML-H-spring_semesester_project-nJO6SWcT/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (20) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3d06182ee268434ba8d475ad3291253e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 62.9106 seconds\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "trainer = pl.Trainer(max_epochs=100)\n",
    "# trainer.fit(model=autoencoder, train_dataloaders=train_loader)\n",
    "with Timer():\n",
    "    trainer.fit(autoencoder, train_dataloaders=train_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-08-04T09:35:36.524861038Z",
     "start_time": "2023-08-04T09:34:33.503171069Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-08-04T09:35:47.027055915Z",
     "start_time": "2023-08-04T09:35:46.992795781Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# test_dataset = SimpleRandomDataset(10000, transform=transforms.ToTensor())\n",
    "# test_loader = DataLoader(test_dataset, batch_size=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-08-04T09:35:47.288760250Z",
     "start_time": "2023-08-04T09:35:47.269203346Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "test_dataset = SimpleRandomBitDataset(10000, transform=transforms.ToTensor())\n",
    "test_loader = DataLoader(test_dataset, batch_size=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-08-04T09:35:47.733343944Z",
     "start_time": "2023-08-04T09:35:47.571507239Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original x: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         0., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
      "         1., 1., 1., 1.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
      "         1., 1., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
      "         1., 1., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
      "         1., 1., 1., 1.]], dtype=torch.float64)\n",
      "Reconstructed x: tensor([[ 0.467,  0.053,  0.081,  0.132,  0.264,  0.423,  0.311,  0.093,  0.538,  0.636,  0.473,  0.628,  0.506,  0.397,  0.558,  0.088,  0.197,  0.466,  0.550,  0.704,  0.504,  0.777,  0.541,  0.738,  0.572,  0.607,  0.678,  0.390,  0.662,  0.524,\n",
      "          0.472,  0.375,  0.222,  0.531,  0.409,  0.530,  0.373,  0.525,  0.422,  0.436,  0.634,  0.525,  0.722,  0.316,  0.342,  0.563,  0.468,  0.553,  0.070,  0.003, -0.021,  0.859, -0.011, -0.043,  0.495,  0.308,  0.300,  0.938,  0.069,  0.070,\n",
      "          0.066,  0.069,  0.061,  0.076],\n",
      "        [ 0.467,  0.053,  0.081,  0.132,  0.264,  0.423,  0.311,  0.093,  0.538,  0.636,  0.473,  0.628,  0.506,  0.397,  0.558,  0.088,  0.197,  0.466,  0.550,  0.704,  0.504,  0.777,  0.541,  0.738,  0.572,  0.607,  0.678,  0.390,  0.662,  0.524,\n",
      "          0.472,  0.375,  0.222,  0.531,  0.409,  0.530,  0.373,  0.525,  0.422,  0.436,  0.634,  0.525,  0.722,  0.316,  0.342,  0.563,  0.468,  0.553,  0.070,  0.003, -0.021,  0.859, -0.011, -0.043,  0.495,  0.308,  0.300,  0.938,  0.069,  0.070,\n",
      "          0.066,  0.069,  0.061,  0.076],\n",
      "        [ 0.560,  0.523,  0.595,  0.561,  0.542,  0.502,  0.434,  0.145,  0.534,  0.588,  0.531,  0.571,  0.575,  0.605,  0.521,  0.604,  0.582,  0.567,  0.571,  0.511,  0.553,  0.539,  0.557,  0.591,  0.574,  0.568,  0.598,  0.539,  0.509,  0.490,\n",
      "          0.616,  0.587,  0.570,  0.555,  0.585,  0.558,  0.554,  0.487,  0.559,  0.566,  0.608,  0.538,  0.498,  0.522,  0.520,  0.620,  0.593,  0.567, -0.007, -0.018, -0.011,  0.043,  0.403,  0.398,  0.569,  0.571,  0.581,  1.024, -0.029, -0.013,\n",
      "         -0.020, -0.016, -0.032, -0.026],\n",
      "        [ 0.560,  0.523,  0.595,  0.561,  0.542,  0.502,  0.434,  0.145,  0.534,  0.588,  0.531,  0.571,  0.575,  0.605,  0.521,  0.604,  0.582,  0.567,  0.571,  0.511,  0.553,  0.539,  0.557,  0.591,  0.574,  0.568,  0.598,  0.539,  0.509,  0.490,\n",
      "          0.616,  0.587,  0.570,  0.555,  0.585,  0.558,  0.554,  0.487,  0.559,  0.566,  0.608,  0.538,  0.498,  0.522,  0.520,  0.620,  0.593,  0.567, -0.007, -0.018, -0.011,  0.043,  0.403,  0.398,  0.569,  0.571,  0.581,  1.024, -0.029, -0.013,\n",
      "         -0.020, -0.016, -0.032, -0.026],\n",
      "        [ 0.561,  0.373,  0.468,  0.446,  0.458,  0.449,  0.391,  0.130,  0.554,  0.681,  0.525,  0.641,  0.599,  0.598,  0.548,  0.486,  0.496,  0.583,  0.604,  0.579,  0.580,  0.659,  0.599,  0.708,  0.629,  0.650,  0.702,  0.516,  0.579,  0.516,\n",
      "          0.647,  0.559,  0.486,  0.585,  0.582,  0.609,  0.542,  0.497,  0.561,  0.566,  0.694,  0.574,  0.591,  0.468,  0.466,  0.667,  0.621,  0.620,  0.045,  0.014,  0.004,  0.043,  0.364,  0.344,  0.592,  0.519,  0.542,  0.969,  0.021,  0.039,\n",
      "          0.031,  0.037,  0.018,  0.024],\n",
      "        [ 0.561,  0.373,  0.468,  0.446,  0.458,  0.449,  0.391,  0.130,  0.554,  0.681,  0.525,  0.641,  0.599,  0.598,  0.548,  0.486,  0.496,  0.583,  0.604,  0.579,  0.580,  0.659,  0.599,  0.708,  0.629,  0.650,  0.702,  0.516,  0.579,  0.516,\n",
      "          0.647,  0.559,  0.486,  0.585,  0.582,  0.609,  0.542,  0.497,  0.561,  0.566,  0.694,  0.574,  0.591,  0.468,  0.466,  0.667,  0.621,  0.620,  0.045,  0.014,  0.004,  0.043,  0.364,  0.344,  0.592,  0.519,  0.542,  0.969,  0.021,  0.039,\n",
      "          0.031,  0.037,  0.018,  0.024],\n",
      "        [ 0.512,  0.727,  0.695,  0.619,  0.449,  0.192,  0.062, -0.015,  0.467,  0.367,  0.509,  0.390,  0.482,  0.550,  0.459,  0.748,  0.690,  0.487,  0.492,  0.392,  0.471,  0.288,  0.436,  0.341,  0.416,  0.344,  0.346,  0.560,  0.390,  0.419,\n",
      "          0.478,  0.568,  0.677,  0.464,  0.518,  0.424,  0.518,  0.462,  0.523,  0.513,  0.386,  0.429,  0.312,  0.592,  0.586,  0.458,  0.463,  0.427,  0.987,  0.956,  0.841,  0.249,  0.827,  0.841,  0.493,  0.615,  0.533,  0.019,  0.975,  0.985,\n",
      "          0.980,  0.984,  0.973,  0.975],\n",
      "        [ 0.512,  0.727,  0.695,  0.619,  0.449,  0.192,  0.062, -0.015,  0.467,  0.367,  0.509,  0.390,  0.482,  0.550,  0.459,  0.748,  0.690,  0.487,  0.492,  0.392,  0.471,  0.288,  0.436,  0.341,  0.416,  0.344,  0.346,  0.560,  0.390,  0.419,\n",
      "          0.478,  0.568,  0.677,  0.464,  0.518,  0.424,  0.518,  0.462,  0.523,  0.513,  0.386,  0.429,  0.312,  0.592,  0.586,  0.458,  0.463,  0.427,  0.987,  0.956,  0.841,  0.249,  0.827,  0.841,  0.493,  0.615,  0.533,  0.019,  0.975,  0.985,\n",
      "          0.980,  0.984,  0.973,  0.975],\n",
      "        [ 0.517,  0.858,  0.804,  0.717,  0.531,  0.274,  0.120, -0.004,  0.446,  0.284,  0.516,  0.327,  0.457,  0.553,  0.435,  0.848,  0.764,  0.474,  0.460,  0.340,  0.445,  0.194,  0.394,  0.239,  0.375,  0.271,  0.249,  0.580,  0.334,  0.393,\n",
      "          0.445,  0.593,  0.745,  0.435,  0.523,  0.371,  0.530,  0.453,  0.514,  0.514,  0.305,  0.390,  0.237,  0.647,  0.637,  0.419,  0.441,  0.385,  1.045,  1.029,  0.925,  0.669,  0.685,  0.730,  0.474,  0.655,  0.582, -0.027,  1.041,  1.042,\n",
      "          1.040,  1.040,  1.036,  1.044],\n",
      "        [ 0.517,  0.858,  0.804,  0.717,  0.531,  0.274,  0.120, -0.004,  0.446,  0.284,  0.516,  0.327,  0.457,  0.553,  0.435,  0.848,  0.764,  0.474,  0.460,  0.340,  0.445,  0.194,  0.394,  0.239,  0.375,  0.271,  0.249,  0.580,  0.334,  0.393,\n",
      "          0.445,  0.593,  0.745,  0.435,  0.523,  0.371,  0.530,  0.453,  0.514,  0.514,  0.305,  0.390,  0.237,  0.647,  0.637,  0.419,  0.441,  0.385,  1.045,  1.029,  0.925,  0.669,  0.685,  0.730,  0.474,  0.655,  0.582, -0.027,  1.041,  1.042,\n",
      "          1.040,  1.040,  1.036,  1.044]], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [00:00, 2904.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.0\n",
      "square sum: 12.320128776078027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sum_dif = 0\n",
    "acc = 0\n",
    "n = 0\n",
    "with torch.no_grad():\n",
    "    # for batch in tqdm(test_loader):\n",
    "    n += 1\n",
    "    for index, batch in tqdm(enumerate(test_loader)):\n",
    "        batch = batch[:, 0, :]\n",
    "        x, y = batch.reshape(2, len(batch), 64)\n",
    "        y_hat = autoencoder.encoder(x)\n",
    "        my_acc = (y == torch.round(y_hat)).all(axis=1).sum().item() / y.shape[0]\n",
    "        acc += my_acc \n",
    "        if index == 0:\n",
    "            print(\"Original x:\", x)\n",
    "            print(\"Reconstructed x:\", y_hat)\n",
    "        sum_dif += ((x - y_hat) ** 2).sum()\n",
    "print(f\"accuracy: {acc /n}\")\n",
    "print(f\"square sum: {(sum_dif / (index + 1) / 10)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-08-04T09:35:48.293040401Z",
     "start_time": "2023-08-04T09:35:47.901644325Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avarage diff: tensor([[[0.510, 0.482, 0.493, 0.466, 0.432, 0.371, 0.268, 0.070, 0.503, 0.491, 0.504, 0.499, 0.505, 0.514, 0.501, 0.515, 0.515, 0.496, 0.522, 0.514, 0.496, 0.491, 0.492, 0.511, 0.497, 0.471, 0.497, 0.501, 0.495, 0.465, 0.503, 0.513, 0.517, 0.500,\n",
      "          0.500, 0.480, 0.482, 0.487, 0.495, 0.500, 0.506, 0.479, 0.476, 0.494, 0.498, 0.525, 0.491, 0.493, 0.409, 0.374, 0.326, 0.454, 0.393, 0.400, 0.507, 0.510, 0.482, 0.601, 0.397, 0.405, 0.400, 0.404, 0.392, 0.400],\n",
      "         [0.510, 0.482, 0.493, 0.466, 0.432, 0.371, 0.268, 0.070, 0.503, 0.491, 0.504, 0.499, 0.505, 0.514, 0.501, 0.515, 0.515, 0.496, 0.522, 0.514, 0.496, 0.491, 0.492, 0.511, 0.497, 0.471, 0.497, 0.501, 0.495, 0.465, 0.503, 0.513, 0.517, 0.500,\n",
      "          0.500, 0.480, 0.482, 0.487, 0.495, 0.500, 0.506, 0.479, 0.476, 0.494, 0.498, 0.525, 0.491, 0.493, 0.409, 0.374, 0.326, 0.454, 0.393, 0.400, 0.507, 0.510, 0.482, 0.601, 0.397, 0.405, 0.400, 0.404, 0.392, 0.400]]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "n = 1000\n",
    "s = 0\n",
    "c = 0\n",
    "with torch.no_grad():\n",
    "    for val in test_dataset:\n",
    "        c += 1\n",
    "        s += autoencoder.encoder(val)\n",
    "print(f\"avarage diff: {s / c}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-08-04T09:35:37.948033872Z",
     "start_time": "2023-08-04T09:35:37.115222125Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.80536684 10.03165292 11.11330816 12.15354934 10.51204465 11.11122781\n",
      " 13.25821883 13.2355479  11.95338963 12.04109997 11.85705404 12.31332344\n",
      " 13.15148366 11.80520051 13.50976217 13.00526406 12.09180656 13.0359287\n",
      " 12.58642785 13.3036195  12.17891203 12.92736482 12.57799643 13.09145882\n",
      " 13.32986844  9.61204512 13.00832978 13.35025851 13.47797857 12.63046657\n",
      " 13.16503786 13.33140043 10.18101471 12.42854152 13.15225413 13.10879907\n",
      " 12.81095544 13.04799235 13.00009067 12.98554396 12.17018801 11.76217339\n",
      " 13.48539785 13.0423147  13.48100122 13.06633251 12.39957927 13.14063366\n",
      " 13.43460878 13.26836372  9.9775123  12.63980476 11.9025952  12.81638807\n",
      " 13.06594308 13.05116897 13.01563688 12.26896345 13.37676964 13.30352229\n",
      " 13.23204777 12.6082651  13.27649656 12.41895439 10.36455637 12.53265375\n",
      " 12.24091851 13.43028346 13.06880617 13.13144517 13.15354126 13.22273933\n",
      " 12.54876306 12.09088759 13.3941249  11.3568514  13.24919882 13.18387379\n",
      " 13.01389294 12.93813746 11.77303577 13.25052615 12.3760285  13.17438472\n",
      " 12.87848585 13.19592506 13.43942662 13.01673648 13.54480315 13.04332364\n",
      " 13.13367779 12.66058633 13.32617528 12.87420721 13.19967614 13.18355506\n",
      " 12.3812463  12.57480908 13.07924853 12.43199905  9.61530428 13.81480762\n",
      " 13.481612   14.66871515 13.10153626 11.95204548 14.08070099 14.78776649\n",
      " 13.52997737 14.86240363 14.1541252  14.81393898 13.78082066 14.12842952\n",
      " 13.08386216 14.36581514 14.13442881 14.71001697 14.70584566 14.90858561\n",
      " 13.87477764 14.33846544 14.87091539 14.66971934 14.41165682 10.86557776\n",
      " 14.03442361 14.26514631 11.11126546 14.33549641 11.92946109 14.87812062\n",
      " 13.9801334  14.21308227 14.74331639 14.85234462 13.41717806 14.59144386\n",
      " 14.29409129 13.62075315 14.28689362 14.56319673 14.82925528 14.75110532\n",
      " 13.4651906  14.76975839 14.76340325 14.83891559 14.94952551 15.09781037\n",
      " 12.98165791 14.53756369 13.58432228 13.56092411 14.98688412 13.38257653\n",
      " 14.78544405 14.47568713 14.26899648 14.82678905 12.00468526 13.3205492\n",
      " 13.95826257 14.11683124 11.94145135 13.78291155 14.04751185 13.46564909\n",
      " 13.58140638 13.02828885 14.05333049 13.20190498 14.05490273 14.34722004\n",
      " 13.48265959 13.42519811 12.16018474 13.32892007 13.04789873 13.77623549\n",
      " 12.68368543 14.14950959 14.33672781 13.99362466 14.18557481 13.76357907\n",
      " 13.66876274 12.75883205 13.7669108  13.86562657 14.12351515 14.12655593\n",
      " 12.79241386 14.17746076 13.63727766 13.68617233 13.94077964 14.18329827\n",
      " 13.60071024 11.64067135 10.96286121 12.88592912 13.66860631 11.52390992\n",
      " 14.19299575 12.99060987 14.08829703 13.30439936 11.65041785 14.09755257\n",
      " 13.43469886 13.92741819 14.11453667 13.59358273 12.8094211  13.76357066\n",
      " 13.74297901 13.34571758 13.87967271 13.27749027 14.13707187 14.0066257\n",
      " 14.1228412  13.69916823 13.51561762 12.37486059 13.84697807 12.77154933\n",
      " 13.09204691 13.44800775 13.06792472 13.48724082 14.09509227 13.11596224\n",
      " 13.66446926 13.28740356 13.22098861 13.8178757  14.01452386 13.90287223\n",
      " 12.41152647 14.22914317 13.651658   13.91917977 13.89175612 14.09447945\n",
      " 13.72988762 13.55176935 14.25677082 13.37580063 10.29677749 10.90761786\n",
      " 12.90378514 13.15088849 13.79299323 12.71933383  9.63838643 13.61200293\n",
      " 12.03440957 13.38616002 13.07161883 13.5597517  13.36942101 13.44487319\n",
      " 12.69385699 12.72395456 12.79233798 12.85500326 13.67023148 13.54987062\n",
      " 13.56035742 13.35586816 12.80962258 13.37540991 13.11885604 12.58859194\n",
      " 12.97237274 13.50002717 13.39971281 12.61284374 12.37571166 10.65398164\n",
      " 13.53263731 12.76162419 13.40289223 13.31643152 12.84478925 13.18535116\n",
      " 12.71252257 13.02421057 12.56898354 13.03761103 12.38952937 13.21876411\n",
      " 12.45809347 13.1123095  12.67026272 13.16321604 13.32081904 13.20225495\n",
      " 13.32339632 13.74265088 15.15051144 14.98155173 14.50273364 14.69225486\n",
      " 13.25283673 15.01453755 15.06852401 15.07001268 14.35625496 15.18661337\n",
      " 14.97226349 12.71835069 14.96627802 14.82186686 14.9421172  15.10916862\n",
      " 14.16565884 14.90149156 12.88880461 13.90225992 15.48515494 14.99389354\n",
      " 15.13176871 13.22967249 13.87482662 13.39708406 11.14690315 12.98148437\n",
      " 13.94509345 13.3882667  13.58895807 13.83738874 13.28209533 13.65284104\n",
      " 13.24817586 12.80985601 12.15115383 13.67882881 13.76685496 13.83343868\n",
      " 13.23191163 12.44783849 13.86344562 13.21810698 12.68664268 13.39807902\n",
      " 13.40261945 11.57362619 13.67935143 13.69150924 12.71872001 12.59110302\n",
      " 13.13143325 13.54339168 12.91714049 13.33301823 13.28919841 11.51782204\n",
      " 11.74247239 13.38894371 13.09077903 12.79605941 13.606976   13.56375595\n",
      " 13.54006371 13.55858829 12.6497267  13.52950522 13.36043786 13.52060445\n",
      " 13.48112138 13.10458138 11.34454162 11.95051847 11.13455784 11.92755679\n",
      " 11.78968821 12.55008197 12.64675434 11.93069523 12.12528666 12.27944099\n",
      " 11.00612341 11.15756396 11.1922424  11.71980408 11.30978612 12.39499011\n",
      " 12.14058812 12.60676056 12.04378377 11.67809957 12.4769217  12.60117542\n",
      " 11.42103915 12.15109735 10.43990277 11.57712084  8.59768062 11.73260049\n",
      " 11.52216397 11.22756633 11.41340168 12.24121022  8.85070208 11.257545\n",
      " 12.14120794 11.81654996 11.70434894 12.47222597 12.67310515 12.79866829\n",
      " 12.3335823  12.81474952 11.74744121 12.68299696 12.86407512 12.74111057\n",
      " 12.29566724 12.3578075  12.51637673 11.78161824 12.83463289 12.22058889\n",
      " 11.65252525 12.44827965 12.45285376 12.56044829 12.81530184 12.47659431\n",
      " 12.62949772 12.3824879  12.42073541 12.0544408  12.69539651 10.22163241\n",
      " 12.09636382 12.35067446 12.3107777  11.95219439 11.99940294 12.45362143\n",
      " 12.1118865  12.26917743 11.51697104 12.08814125 12.2297088  12.25373651\n",
      " 11.55039471 12.19062648 12.33025573 11.68897875 12.04982439 11.7604549\n",
      " 11.31361937 12.26583571 12.09571106 12.15902997 12.63181139 12.24950032\n",
      " 11.68287584 11.86094458 11.98683618 12.17313923 12.39883319 12.17900715\n",
      " 12.17299431 11.96478108 11.52104538 11.76850277 11.97956722 11.98478673\n",
      " 12.01623637 11.48452734 11.49515477 11.7364793  12.33623254 11.86198771\n",
      " 12.20666929 12.30887996 12.01797345 12.29853959 12.39735951 12.6181395\n",
      " 12.62222925 11.0299137  12.99743206 12.1059359  12.81367612 12.67397359\n",
      " 11.72659868 11.1438266  11.47572991 12.64919758 12.81246668 11.23695872\n",
      " 12.2613704  10.99838802 10.22286575 11.84380487 10.51085324 12.57235789\n",
      " 12.24048387 10.92896323 10.5820585  11.32397428 12.39165973 12.53137319\n",
      " 11.54037478 12.77656048 10.42067704 12.40926813 12.52950811 12.57124246\n",
      " 11.91412938 11.60208816 12.09091356 12.22049713 11.44771995 12.07171583\n",
      " 11.41069097 11.03068862 11.23983834 11.9938249  12.02700851 11.95949403\n",
      " 11.41647535 11.43496534 11.89247385 11.87542556 11.8759963  12.46518633\n",
      " 11.96673528 12.09161271 11.59165918 11.20712967 11.95921458 12.51800541\n",
      " 12.14318105 12.15513664 12.16987446 11.69836811 11.17431946 11.85182526\n",
      " 11.72040585 12.13197365 12.10002568 11.57433832 12.13177507 12.49904271\n",
      " 11.91056532 11.3725348  12.4025989  12.34541209 12.3922916  11.85538707\n",
      " 12.15601009 12.46668145 12.19087418 12.27227041  9.47261172 12.76975282\n",
      " 11.51996912 12.61868492 12.12086365 12.69232437 12.41257714 12.69221977\n",
      " 12.42552836 12.27435981 12.0349276  11.63533953 12.38719012 12.65998498\n",
      " 11.14865939 12.34996761 11.91902455 12.45714183 12.54903167 12.81090114\n",
      " 12.30957827 11.94427726 12.66674707 12.35665757 12.57748023 12.217818\n",
      " 12.22364913 11.43723746 11.7985127  11.99541739 10.64608719  8.90251861\n",
      " 11.76686489 11.52937872 10.71188435 11.38142828 11.5845475   8.9305287\n",
      " 11.23772523 10.03391868 11.7761488  11.79587178 12.64589895 12.43506268\n",
      " 11.83720051 12.46624058 12.22607337 11.98467411 12.53178223 12.03367418\n",
      " 11.05792778 11.41112554 11.8276633  11.90505013 11.74045185 12.04716961\n",
      " 12.61353579 12.78317049 12.60535779 12.33047567 12.49497516 12.0302355\n",
      " 12.16349501 11.36751612 12.96999583 13.43359946 13.8468086  14.14414056\n",
      " 13.87865781 13.49325585 13.53407005 13.75350674 14.16432714 14.2632124\n",
      " 13.64752876 13.13230307 14.0162744  13.10631947 10.89767977 13.45451147\n",
      " 13.65834532 13.67123513 13.70060457 13.63404879 13.39747136 13.54357312\n",
      " 13.94183919 14.24107748 12.08723513 13.93060348 13.59221347 13.52021199\n",
      " 14.1808607  14.55649122 12.7732523  13.72400353 14.33281077 14.45597273\n",
      " 14.14118849 13.29292959 13.18362921 14.02129408 14.14600831 13.39073993\n",
      " 14.37029783 14.32735655 14.21818429 14.52584309 13.85783703 12.46645277\n",
      " 13.42192739 14.71316838 14.14841499 15.24851933 14.90176014 15.39700815\n",
      " 14.22416834 13.39731937 14.49220638 14.11389975 15.14359946 15.04246447\n",
      " 14.84906605 15.04213849 13.27730631 14.84095043 14.90273963 14.45865709\n",
      " 14.86582512 15.10927406 15.07834974 13.70710467 14.86717126 14.72631426\n",
      " 14.99841905 14.80934803 14.04963169 13.77086523 13.30314354 13.25917756\n",
      " 13.54885073 13.10826577 12.94679009 12.62628647 13.11040464 12.91931392\n",
      " 13.22009374 13.12728762 13.48509826 13.29974756 13.07376472 13.19824101\n",
      " 13.62161768 13.57078572 13.22873788 13.84677456 11.49826691 12.99932839\n",
      " 12.26097865 13.69574171 13.608792   13.37967318 13.07245634 13.28852975\n",
      " 13.31063892 13.33734993 13.46184178 13.64283999 13.66541193 13.84237548\n",
      " 13.2517329  13.19271467 13.23196541 13.28301165 13.4041642  13.3278879\n",
      " 13.8269467  13.57458389 13.42031744 12.73215085 13.8465433  10.67685827\n",
      " 13.22743432 13.89435555 13.5933556  13.40206523 11.41706476 11.20012057\n",
      " 13.37821504 14.18259439 13.16097707 13.71247512 13.92715319 14.11357292\n",
      " 13.75954403 13.9731344  14.28722032 13.00621927 14.02755264 14.0173666\n",
      " 13.60450877 13.46954832 13.55814506 13.61811421 13.53630612 14.26160835\n",
      " 13.12907383 13.37030602 13.19944753 13.53232509 12.87724117 14.06814368\n",
      " 12.88223424 13.87354057 13.44277488 14.04834056 14.08548054 14.28586678\n",
      " 13.61797413 13.90764059 13.70407425 14.05296273 13.70200029 13.10975861\n",
      " 13.61277865 14.20982502 14.10368752 13.77336105 14.16342963 12.35998208\n",
      " 13.22630172 13.97758727 13.35820381 14.29315845 11.95507571 13.95021494\n",
      " 13.19429516 11.70867337 10.46868061 12.1538545  13.32114564 13.53479995\n",
      " 12.92056405 13.2805666  13.49490187 12.04179356 13.32997459 13.46302696\n",
      " 13.55307856 13.01132117 11.97205769 13.24547766 13.55934476 13.27184348\n",
      " 13.13341736 13.51382566 13.29798807 13.11400603 13.03346886 13.09343649\n",
      " 13.22237933 11.59721132 13.27367918 12.69448922 13.61624981 13.73800313\n",
      " 12.74106392 13.65147166 12.98243273 13.0400521  12.29282702 13.51838129\n",
      " 13.46127275 12.71896891 13.63911992 13.61016834 13.17167065 12.28183778\n",
      " 13.45522495 13.34195169 13.45357036 12.82115545 12.62080625 13.6026947\n",
      " 12.87916755 13.56836854 12.75966837 12.61896247 13.03420477 12.63195697\n",
      " 12.8727836  13.24237528 13.10134669 12.77214421 12.52763948 12.28997558\n",
      " 12.78425549 12.89893048 12.18960533 12.8741434  13.06152435 13.00162093\n",
      " 12.84991034 12.86575787 12.60299729 12.91498333 12.86237865 11.44445988\n",
      " 12.57659709 11.14329857 12.66219504 12.23716563  9.86995447 12.32828349\n",
      " 12.52657549 13.21488522 12.86522176 12.50060452 12.69091936 12.89820039\n",
      " 12.35707449 12.87259064 12.79577265 12.26516377 12.649504   11.72045275\n",
      " 13.05466134 12.67366645 12.82909181 12.61515349 12.96713843 12.27339375\n",
      " 10.99104457 11.24973043 12.40964217 12.20061025 11.79808907  9.85029132\n",
      " 11.99541362 13.32909326 12.89167228 12.19091325 13.12974989 13.2034656\n",
      " 13.0705047  13.35029931 12.96385515 13.25298633 13.26784668 13.72145446\n",
      " 12.6926724  13.59539926 13.31000046 12.75519644 13.17068375 12.91510835\n",
      " 13.33532967 12.03919627 13.16838136 12.68671299 13.41497994 13.48066589\n",
      " 12.08580833 13.29780052 12.62847032 13.01294116 13.05122935 13.45655298\n",
      " 13.36372856 13.39426431 13.58481223 12.84694651 12.98636553 10.5015168\n",
      " 12.85898515 13.08318425 13.00648692 13.4981828  13.17169345 13.42843429\n",
      " 12.79650987 12.94848387 13.18886959 13.43737141 12.78339492 12.11425471\n",
      " 12.9588225  10.86062113 13.18599074 13.43691266 13.38457333 11.96131777\n",
      " 13.3389909  13.61902449 13.37060375 13.61081564 12.38992708 12.38746666\n",
      " 12.73000311 13.33945759 13.00303944 13.14965644 13.30467487 13.30625244\n",
      " 12.80965988 10.39652196 13.26087063 13.40280297 13.0913271  13.42508051\n",
      " 13.41640364 13.38249803 10.49538791 13.39481036 13.24542669 13.06388195\n",
      " 12.90307564 12.65766252 13.46524703 13.12265812 13.29804324 12.53755727\n",
      " 13.20562579 13.53937915 12.43546945 13.27897784 12.82285093 12.42728589\n",
      " 12.25552481 12.34828212 13.21143667 13.51951209 11.58801706 11.25107957\n",
      " 12.26233403 11.64320874 10.79546745  4.77962887]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([FloatBitsConverter.get_float_bits_in_memory(val) for val in np.linspace(-20, 20, 1000)])\n",
    "x = x.reshape(len(x), 64)\n",
    "x = torch.tensor(x)\n",
    "\n",
    "y = autoencoder.encoder(x)\n",
    "\n",
    "x = x.detach().numpy()\n",
    "y = y.detach().numpy()\n",
    "differ = (x - y) ** 2\n",
    "print(differ.sum(axis=1))\n",
    "x_to_plot = [FloatBitsConverter.get_float_by_bits(val) for val in x]\n",
    "y_to_plot = [FloatBitsConverter.get_float_by_bits(val) for val in y]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-08-04T09:35:38.017635486Z",
     "start_time": "2023-08-04T09:35:37.961859364Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[19], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mplotly\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexpress\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpx\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "import plotly.express as px"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-08-04T09:35:38.193468923Z",
     "start_time": "2023-08-04T09:35:37.988375061Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(x[0].astype(int))\n",
    "# print(y[0])\n",
    "print(y[0].round().astype(int))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "start_time": "2023-08-04T09:35:38.167276953Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print(x_to_plot)\n",
    "print(y_to_plot)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "start_time": "2023-08-04T09:35:38.167710674Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = px.line(x=x_to_plot, y=y_to_plot)\n",
    "fig.show()\n",
    "\n",
    "fig = px.line(x=x_to_plot, y=differ.sum(axis=1))\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "start_time": "2023-08-04T09:35:38.167995490Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
