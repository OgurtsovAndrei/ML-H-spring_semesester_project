{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-lightning in c:\\users\\ogurc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.0.4)\n",
      "Requirement already satisfied: numpy>=1.17.2 in c:\\users\\ogurc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pytorch-lightning) (1.25.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\ogurc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pytorch-lightning) (2.0.1+cu117)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in c:\\users\\ogurc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pytorch-lightning) (4.64.0)\n",
      "Requirement already satisfied: PyYAML>=5.4 in c:\\users\\ogurc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pytorch-lightning) (6.0)\n",
      "Requirement already satisfied: fsspec[http]>2021.06.0 in c:\\users\\ogurc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pytorch-lightning) (2023.6.0)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in c:\\users\\ogurc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pytorch-lightning) (1.0.0)\n",
      "Requirement already satisfied: packaging>=17.1 in c:\\users\\ogurc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pytorch-lightning) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\ogurc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pytorch-lightning) (4.4.0)\n",
      "Requirement already satisfied: lightning-utilities>=0.7.0 in c:\\users\\ogurc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pytorch-lightning) (0.9.0)\n",
      "Requirement already satisfied: requests in c:\\users\\ogurc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (2.31.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\ogurc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (3.8.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\ogurc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging>=17.1->pytorch-lightning) (3.0.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\ogurc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->pytorch-lightning) (3.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\ogurc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->pytorch-lightning) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\ogurc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->pytorch-lightning) (3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ogurc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->pytorch-lightning) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\ogurc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.57.0->pytorch-lightning) (0.4.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\ogurc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (21.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\ogurc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (2.0.12)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\ogurc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\ogurc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\ogurc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\ogurc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\ogurc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ogurc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch>=1.11.0->pytorch-lightning) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ogurc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ogurc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ogurc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2022.5.18.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\ogurc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch>=1.11.0->pytorch-lightning) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "!pip install pytorch-lightning"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ogurc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "activation_fun = nn.ReLU()\n",
    "layer_sizes = [1, 8, 8, 8, 1]\n",
    "\n",
    "\n",
    "class SamenessModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.run_counter = 0\n",
    "\n",
    "        layers_list = []\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            layers_list.append(nn.Linear(layer_sizes[i], layer_sizes[i + 1]))\n",
    "            if i != len(layer_sizes) - 2:\n",
    "                layers_list.append(activation_fun)\n",
    "\n",
    "        self.l1 = nn.Sequential(*layers_list)\n",
    "        # self.layers = layers_list\n",
    "        print(self)\n",
    "\n",
    "        for layer in self.l1:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                layer.weight = nn.Parameter(layer.weight.double())\n",
    "                layer.bias = nn.Parameter(layer.bias.double())\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.run_counter += 1\n",
    "        return self.l1(x.double())  # <--- Вылетает ошибка тут\n",
    "\n",
    "\n",
    "# class Decoder(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.l1 = nn.Sequential(nn.Linear(3, 64), nn.ReLU(), nn.Linear(64, 28 * 28))\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         return self.l1(x)\n",
    "\n",
    "\n",
    "class SamenessAutoEncoder(pl.LightningModule):\n",
    "    def __init__(self, encoder: SamenessModule):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        # print(\"\\n\" + \"-\" * 20)\n",
    "        # print(f\"batch: {batch}\")\n",
    "        # print(\"-\" * 20)\n",
    "        x, y = batch[0][0]\n",
    "        print(f\"batch:{'-' * 10}\\n{x}\\n{y}\\n{'-' * 20}\")\n",
    "        x_hat = self.encoder(x)\n",
    "        loss = F.mse_loss(x_hat, x)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import os\n",
    "import torch.random\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "# from notebooks.data.SimpleRandomDataset import SimpleRandomDataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "class SimpleRandomDataset(Dataset):\n",
    "    \"\"\"Simplest random dataset\"\"\"\n",
    "\n",
    "    def __init__(self, n, left_bound=-5, right_bound=5, transform=None):\n",
    "        \"\"\"\n",
    "\n",
    "        :param left_bound:\n",
    "        :param right_bound:\n",
    "        \"\"\"\n",
    "        self.size = n\n",
    "        self.transform = transform\n",
    "        random_array = np.random.rand(n) * (right_bound - left_bound) + left_bound\n",
    "        random_array = random_array.reshape((len(random_array), 1))\n",
    "        self.data = np.array([random_array, random_array])\n",
    "        # self.data = pd.DataFrame({\n",
    "        #     'X': random_array,\n",
    "        #     'y': random_array,\n",
    "        # })\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        print(idx)\n",
    "        # img_name = os.path.join(self.root_dir,\n",
    "        #                         self.landmarks_frame.iloc[idx, 0])\n",
    "        # image = io.imread(img_name)\n",
    "        # landmarks = self.landmarks_frame.iloc[idx, 1:]\n",
    "        # landmarks = np.array([landmarks])\n",
    "        # landmarks = landmarks.astype('float').reshape(-1, 2)\n",
    "        # sample = {'image': image, 'landmarks': landmarks}\n",
    "        #\n",
    "        # if self.transform:\n",
    "        #     sample = self.transform(sample)\n",
    "        #\n",
    "        # return sample\n",
    "        # pass\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        # if isinstance(idx, int):\n",
    "        #     idx = [idx]\n",
    "\n",
    "        # X = self.data[idx]\n",
    "        # y = self.data[idx]\n",
    "        # sample = {\"X\": X, \"y\": y}\n",
    "\n",
    "        # sample = self.data.iloc[idx].to_numpy()\n",
    "        sample = self.data[:, idx]\n",
    "        # sample = np.array([X, y])\n",
    "\n",
    "        # FIXME:\n",
    "        # if idx is int:\n",
    "        #     sample = self.data.iloc[idx].to_list()\n",
    "        #     sample = [[sample[0]], [sample[1]]]\n",
    "        # else:\n",
    "        # sample = self.data.iloc[idx].to_numpy().T\n",
    "\n",
    "        # print(\"\\n\" + \"-\" * 20)\n",
    "        # print(f\"sample: {sample}\")\n",
    "        # print(\"-\" * 20)\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        # print(f\"sample: {sample}\")\n",
    "        # print(\"-\" * 20)\n",
    "        return sample"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3050 Ti Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type           | Params\n",
      "-------------------------------------------\n",
      "0 | encoder | SamenessModule | 169   \n",
      "-------------------------------------------\n",
      "169       Trainable params\n",
      "0         Non-trainable params\n",
      "169       Total params\n",
      "0.001     Total estimated model params size (MB)\n",
      "C:\\Users\\ogurc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (10) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SamenessModule(\n",
      "  (l1): Sequential(\n",
      "    (0): Linear(in_features=1, out_features=8, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=8, out_features=8, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=8, out_features=8, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=8, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0:   0%|          | 0/10 [00:00<?, ?it/s] 0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "batch:----------\n",
      "tensor([0.2073], device='cuda:0', dtype=torch.float64)\n",
      "tensor([0.2073], device='cuda:0', dtype=torch.float64)\n",
      "--------------------\n",
      "Epoch 0:  10%|█         | 1/10 [00:00<00:00, 76.91it/s, v_num=106]10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "batch:----------\n",
      "tensor([-3.6697], device='cuda:0', dtype=torch.float64)\n",
      "tensor([-3.6697], device='cuda:0', dtype=torch.float64)\n",
      "--------------------\n",
      "Epoch 0:  20%|██        | 2/10 [00:00<00:00, 83.31it/s, v_num=106]20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "batch:----------\n",
      "tensor([-4.7279], device='cuda:0', dtype=torch.float64)\n",
      "tensor([-4.7279], device='cuda:0', dtype=torch.float64)\n",
      "--------------------\n",
      "Epoch 0:  30%|███       | 3/10 [00:00<00:00, 88.19it/s, v_num=106]30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "batch:----------\n",
      "tensor([-2.9540], device='cuda:0', dtype=torch.float64)\n",
      "tensor([-2.9540], device='cuda:0', dtype=torch.float64)\n",
      "--------------------\n",
      "Epoch 0:  40%|████      | 4/10 [00:00<00:00, 90.89it/s, v_num=106]40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "batch:----------\n",
      "tensor([4.1632], device='cuda:0', dtype=torch.float64)\n",
      "tensor([4.1632], device='cuda:0', dtype=torch.float64)\n",
      "--------------------\n",
      "Epoch 0:  50%|█████     | 5/10 [00:00<00:00, 92.59it/s, v_num=106]50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "batch:----------\n",
      "tensor([-4.1822], device='cuda:0', dtype=torch.float64)\n",
      "tensor([-4.1822], device='cuda:0', dtype=torch.float64)\n",
      "--------------------\n",
      "Epoch 0:  60%|██████    | 6/10 [00:00<00:00, 96.77it/s, v_num=106]60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "batch:----------\n",
      "tensor([-3.9292], device='cuda:0', dtype=torch.float64)\n",
      "tensor([-3.9292], device='cuda:0', dtype=torch.float64)\n",
      "--------------------\n",
      "Epoch 0:  70%|███████   | 7/10 [00:00<00:00, 93.32it/s, v_num=106]70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "batch:----------\n",
      "tensor([-0.4463], device='cuda:0', dtype=torch.float64)\n",
      "tensor([-0.4463], device='cuda:0', dtype=torch.float64)\n",
      "--------------------\n",
      "Epoch 0:  80%|████████  | 8/10 [00:00<00:00, 91.95it/s, v_num=106]80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "batch:----------\n",
      "tensor([-2.4969], device='cuda:0', dtype=torch.float64)\n",
      "tensor([-2.4969], device='cuda:0', dtype=torch.float64)\n",
      "--------------------\n",
      "Epoch 0:  90%|█████████ | 9/10 [00:00<00:00, 90.90it/s, v_num=106]90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "batch:----------\n",
      "tensor([-3.7889], device='cuda:0', dtype=torch.float64)\n",
      "tensor([-3.7889], device='cuda:0', dtype=torch.float64)\n",
      "--------------------\n",
      "Epoch 1:   0%|          | 0/10 [00:00<?, ?it/s, v_num=106]         0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "batch:----------\n",
      "tensor([0.2073], device='cuda:0', dtype=torch.float64)\n",
      "tensor([0.2073], device='cuda:0', dtype=torch.float64)\n",
      "--------------------\n",
      "Epoch 1:  10%|█         | 1/10 [00:00<00:00, 83.42it/s, v_num=106]10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "batch:----------\n",
      "tensor([-3.6697], device='cuda:0', dtype=torch.float64)\n",
      "tensor([-3.6697], device='cuda:0', dtype=torch.float64)\n",
      "--------------------\n",
      "Epoch 1:  20%|██        | 2/10 [00:00<00:00, 95.25it/s, v_num=106]20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "batch:----------\n",
      "tensor([-4.7279], device='cuda:0', dtype=torch.float64)\n",
      "tensor([-4.7279], device='cuda:0', dtype=torch.float64)\n",
      "--------------------\n",
      "Epoch 1:  30%|███       | 3/10 [00:00<00:00, 90.91it/s, v_num=106]30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "batch:----------\n",
      "tensor([-2.9540], device='cuda:0', dtype=torch.float64)\n",
      "tensor([-2.9540], device='cuda:0', dtype=torch.float64)\n",
      "--------------------\n",
      "Epoch 1:  40%|████      | 4/10 [00:00<00:00, 90.91it/s, v_num=106]40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "batch:----------\n",
      "tensor([4.1632], device='cuda:0', dtype=torch.float64)\n",
      "tensor([4.1632], device='cuda:0', dtype=torch.float64)\n",
      "--------------------\n",
      "Epoch 1:  50%|█████     | 5/10 [00:00<00:00, 94.35it/s, v_num=106]50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "batch:----------\n",
      "tensor([-4.1822], device='cuda:0', dtype=torch.float64)\n",
      "tensor([-4.1822], device='cuda:0', dtype=torch.float64)\n",
      "--------------------\n",
      "Epoch 1:  60%|██████    | 6/10 [00:00<00:00, 93.75it/s, v_num=106]60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "batch:----------\n",
      "tensor([-3.9292], device='cuda:0', dtype=torch.float64)\n",
      "tensor([-3.9292], device='cuda:0', dtype=torch.float64)\n",
      "--------------------\n",
      "Epoch 1:  70%|███████   | 7/10 [00:00<00:00, 93.33it/s, v_num=106]70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "batch:----------\n",
      "tensor([-0.4463], device='cuda:0', dtype=torch.float64)\n",
      "tensor([-0.4463], device='cuda:0', dtype=torch.float64)\n",
      "--------------------\n",
      "Epoch 1:  80%|████████  | 8/10 [00:00<00:00, 94.12it/s, v_num=106]80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "batch:----------\n",
      "tensor([-2.4969], device='cuda:0', dtype=torch.float64)\n",
      "tensor([-2.4969], device='cuda:0', dtype=torch.float64)\n",
      "--------------------\n",
      "Epoch 1:  90%|█████████ | 9/10 [00:00<00:00, 93.75it/s, v_num=106]90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "batch:----------\n",
      "tensor([-3.7889], device='cuda:0', dtype=torch.float64)\n",
      "tensor([-3.7889], device='cuda:0', dtype=torch.float64)\n",
      "--------------------\n",
      "Epoch 2:   0%|          | 0/10 [00:00<?, ?it/s, v_num=106]         0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "batch:----------\n",
      "tensor([0.2073], device='cuda:0', dtype=torch.float64)\n",
      "tensor([0.2073], device='cuda:0', dtype=torch.float64)\n",
      "--------------------\n",
      "Epoch 2:  10%|█         | 1/10 [00:00<00:00, 77.00it/s, v_num=106]10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "batch:----------\n",
      "tensor([-3.6697], device='cuda:0', dtype=torch.float64)\n",
      "tensor([-3.6697], device='cuda:0', dtype=torch.float64)\n",
      "--------------------\n",
      "Epoch 2:  20%|██        | 2/10 [00:00<00:00, 86.98it/s, v_num=106]20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "batch:----------\n",
      "tensor([-4.7279], device='cuda:0', dtype=torch.float64)\n",
      "tensor([-4.7279], device='cuda:0', dtype=torch.float64)\n",
      "--------------------\n",
      "Epoch 2:  30%|███       | 3/10 [00:00<00:00, 88.25it/s, v_num=106]30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "batch:----------\n",
      "tensor([-2.9540], device='cuda:0', dtype=torch.float64)\n",
      "tensor([-2.9540], device='cuda:0', dtype=torch.float64)\n",
      "--------------------\n",
      "Epoch 2:  40%|████      | 4/10 [00:00<00:00, 86.97it/s, v_num=106]40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "batch:----------\n",
      "tensor([4.1632], device='cuda:0', dtype=torch.float64)\n",
      "tensor([4.1632], device='cuda:0', dtype=torch.float64)\n",
      "--------------------\n",
      "Epoch 2:  50%|█████     | 5/10 [00:00<00:00, 87.72it/s, v_num=106]50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "batch:----------\n",
      "tensor([-4.1822], device='cuda:0', dtype=torch.float64)\n",
      "tensor([-4.1822], device='cuda:0', dtype=torch.float64)\n",
      "--------------------\n",
      "Epoch 2:  60%|██████    | 6/10 [00:00<00:00, 86.96it/s, v_num=106]60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "batch:----------\n",
      "tensor([-3.9292], device='cuda:0', dtype=torch.float64)\n",
      "tensor([-3.9292], device='cuda:0', dtype=torch.float64)\n",
      "--------------------\n",
      "Epoch 2:  70%|███████   | 7/10 [00:00<00:00, 86.43it/s, v_num=106]70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "batch:----------\n",
      "tensor([-0.4463], device='cuda:0', dtype=torch.float64)\n",
      "tensor([-0.4463], device='cuda:0', dtype=torch.float64)\n",
      "--------------------\n",
      "Epoch 2:  80%|████████  | 8/10 [00:00<00:00, 86.96it/s, v_num=106]80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "batch:----------\n",
      "tensor([-2.4969], device='cuda:0', dtype=torch.float64)\n",
      "tensor([-2.4969], device='cuda:0', dtype=torch.float64)\n",
      "--------------------\n",
      "Epoch 2:  90%|█████████ | 9/10 [00:00<00:00, 89.12it/s, v_num=106]90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "batch:----------\n",
      "tensor([-3.7889], device='cuda:0', dtype=torch.float64)\n",
      "tensor([-3.7889], device='cuda:0', dtype=torch.float64)\n",
      "--------------------\n",
      "Epoch 2: 100%|██████████| 10/10 [00:00<00:00, 89.29it/s, v_num=106]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 10/10 [00:00<00:00, 83.34it/s, v_num=106]\n"
     ]
    }
   ],
   "source": [
    "dataset = SimpleRandomDataset(100, transform=transforms.ToTensor())\n",
    "train_loader = DataLoader(dataset, batch_size=10)\n",
    "\n",
    "# model\n",
    "autoencoder = SamenessAutoEncoder(SamenessModule())\n",
    "\n",
    "# train model\n",
    "trainer = pl.Trainer(max_epochs=3)\n",
    "# trainer.fit(model=autoencoder, train_dataloaders=train_loader)\n",
    "trainer.fit(model=autoencoder, train_dataloaders=train_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Original x: tensor([4.2871], dtype=torch.float64)\n",
      "Reconstructed x: tensor([0.1040], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 10%|█         | 1/10 [04:02<36:21, 242.43s/it]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Original x: tensor([2.1474], dtype=torch.float64)\n",
      "Reconstructed x: tensor([0.0318], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 2/10 [04:53<17:18, 129.86s/it]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 30%|███       | 3/10 [06:37<13:46, 118.13s/it]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original x: tensor([1.7623], dtype=torch.float64)\n",
      "Reconstructed x: tensor([0.0358], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 40%|████      | 4/10 [06:39<07:14, 72.37s/it] \u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "Original x: tensor([-4.0209], dtype=torch.float64)\n",
      "Reconstructed x: tensor([-0.2230], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|█████     | 5/10 [06:40<03:52, 46.45s/it]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "Original x: tensor([-3.4434], dtype=torch.float64)\n",
      "Reconstructed x: tensor([-0.1946], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 60%|██████    | 6/10 [06:40<02:03, 30.81s/it]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "Original x: tensor([2.3880], dtype=torch.float64)\n",
      "Reconstructed x: tensor([0.0382], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 70%|███████   | 7/10 [07:27<01:48, 36.02s/it]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "Original x: tensor([0.9512], dtype=torch.float64)\n",
      "Reconstructed x: tensor([0.0331], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [07:29<00:00, 44.97s/it][A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "Original x: tensor([-3.0999], dtype=torch.float64)\n",
      "Reconstructed x: tensor([-0.1758], dtype=torch.float64)\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "Original x: tensor([1.9881], dtype=torch.float64)\n",
      "Reconstructed x: tensor([0.0335], dtype=torch.float64)\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "Original x: tensor([0.5828], dtype=torch.float64)\n",
      "Reconstructed x: tensor([0.0253], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_dataset = SimpleRandomDataset(100, transform=transforms.ToTensor())\n",
    "test_loader = DataLoader(test_dataset, batch_size=10)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # for batch in tqdm(test_loader):\n",
    "    for batch in tqdm(test_loader):\n",
    "        x, y = batch[0][0]\n",
    "        reconstructed_x = autoencoder.encoder(x)\n",
    "        print(\"Original x:\", x)\n",
    "        print(\"Reconstructed x:\", reconstructed_x)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "3"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.encoder.run_counter"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n = 1000\n",
    "s = 0\n",
    "c = 0\n",
    "with torch.no_grad():\n",
    "    for val in test_dataset:\n",
    "        c += 1\n",
    "        s += autoencoder.encoder(val)\n",
    "print(f\"avarage diff: {s / c}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "autoencoder.encoder.run_counter"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "val = torch.tensor([[1], [2], [4], [5]])\n",
    "autoencoder.encoder(val)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-10.          -9.97997998  -9.95995996  -9.93993994  -9.91991992\n",
      "  -9.8998999   -9.87987988  -9.85985986  -9.83983984  -9.81981982\n",
      "  -9.7997998   -9.77977978  -9.75975976  -9.73973974  -9.71971972\n",
      "  -9.6996997   -9.67967968  -9.65965966  -9.63963964  -9.61961962\n",
      "  -9.5995996   -9.57957958  -9.55955956  -9.53953954  -9.51951952\n",
      "  -9.4994995   -9.47947948  -9.45945946  -9.43943944  -9.41941942\n",
      "  -9.3993994   -9.37937938  -9.35935936  -9.33933934  -9.31931932\n",
      "  -9.2992993   -9.27927928  -9.25925926  -9.23923924  -9.21921922\n",
      "  -9.1991992   -9.17917918  -9.15915916  -9.13913914  -9.11911912\n",
      "  -9.0990991   -9.07907908  -9.05905906  -9.03903904  -9.01901902\n",
      "  -8.998999    -8.97897898  -8.95895896  -8.93893894  -8.91891892\n",
      "  -8.8988989   -8.87887888  -8.85885886  -8.83883884  -8.81881882\n",
      "  -8.7987988   -8.77877878  -8.75875876  -8.73873874  -8.71871872\n",
      "  -8.6986987   -8.67867868  -8.65865866  -8.63863864  -8.61861862\n",
      "  -8.5985986   -8.57857858  -8.55855856  -8.53853854  -8.51851852\n",
      "  -8.4984985   -8.47847848  -8.45845846  -8.43843844  -8.41841842\n",
      "  -8.3983984   -8.37837838  -8.35835836  -8.33833834  -8.31831832\n",
      "  -8.2982983   -8.27827828  -8.25825826  -8.23823824  -8.21821822\n",
      "  -8.1981982   -8.17817818  -8.15815816  -8.13813814  -8.11811812\n",
      "  -8.0980981   -8.07807808  -8.05805806  -8.03803804  -8.01801802\n",
      "  -7.997998    -7.97797798  -7.95795796  -7.93793794  -7.91791792\n",
      "  -7.8978979   -7.87787788  -7.85785786  -7.83783784  -7.81781782\n",
      "  -7.7977978   -7.77777778  -7.75775776  -7.73773774  -7.71771772\n",
      "  -7.6976977   -7.67767768  -7.65765766  -7.63763764  -7.61761762\n",
      "  -7.5975976   -7.57757758  -7.55755756  -7.53753754  -7.51751752\n",
      "  -7.4974975   -7.47747748  -7.45745746  -7.43743744  -7.41741742\n",
      "  -7.3973974   -7.37737738  -7.35735736  -7.33733734  -7.31731732\n",
      "  -7.2972973   -7.27727728  -7.25725726  -7.23723724  -7.21721722\n",
      "  -7.1971972   -7.17717718  -7.15715716  -7.13713714  -7.11711712\n",
      "  -7.0970971   -7.07707708  -7.05705706  -7.03703704  -7.01701702\n",
      "  -6.996997    -6.97697698  -6.95695696  -6.93693694  -6.91691692\n",
      "  -6.8968969   -6.87687688  -6.85685686  -6.83683684  -6.81681682\n",
      "  -6.7967968   -6.77677678  -6.75675676  -6.73673674  -6.71671672\n",
      "  -6.6966967   -6.67667668  -6.65665666  -6.63663664  -6.61661662\n",
      "  -6.5965966   -6.57657658  -6.55655656  -6.53653654  -6.51651652\n",
      "  -6.4964965   -6.47647648  -6.45645646  -6.43643644  -6.41641642\n",
      "  -6.3963964   -6.37637638  -6.35635636  -6.33633634  -6.31631632\n",
      "  -6.2962963   -6.27627628  -6.25625626  -6.23623624  -6.21621622\n",
      "  -6.1961962   -6.17617618  -6.15615616  -6.13613614  -6.11611612\n",
      "  -6.0960961   -6.07607608  -6.05605606  -6.03603604  -6.01601602\n",
      "  -5.995996    -5.97597598  -5.95595596  -5.93593594  -5.91591592\n",
      "  -5.8958959   -5.87587588  -5.85585586  -5.83583584  -5.81581582\n",
      "  -5.7957958   -5.77577578  -5.75575576  -5.73573574  -5.71571572\n",
      "  -5.6956957   -5.67567568  -5.65565566  -5.63563564  -5.61561562\n",
      "  -5.5955956   -5.57557558  -5.55555556  -5.53553554  -5.51551552\n",
      "  -5.4954955   -5.47547548  -5.45545546  -5.43543544  -5.41541542\n",
      "  -5.3953954   -5.37537538  -5.35535536  -5.33533534  -5.31531532\n",
      "  -5.2952953   -5.27527528  -5.25525526  -5.23523524  -5.21521522\n",
      "  -5.1951952   -5.17517518  -5.15515516  -5.13513514  -5.11511512\n",
      "  -5.0950951   -5.07507508  -5.05505506  -5.03503504  -5.01501502\n",
      "  -4.99499499  -4.97497497  -4.95495495  -4.93493493  -4.91491491\n",
      "  -4.89489489  -4.87487487  -4.85485485  -4.83483483  -4.81481481\n",
      "  -4.79479479  -4.77477477  -4.75475475  -4.73473473  -4.71471471\n",
      "  -4.69469469  -4.67467467  -4.65465465  -4.63463463  -4.61461461\n",
      "  -4.59459459  -4.57457457  -4.55455455  -4.53453453  -4.51451451\n",
      "  -4.49449449  -4.47447447  -4.45445445  -4.43443443  -4.41441441\n",
      "  -4.39439439  -4.37437437  -4.35435435  -4.33433433  -4.31431431\n",
      "  -4.29429429  -4.27427427  -4.25425425  -4.23423423  -4.21421421\n",
      "  -4.19419419  -4.17417417  -4.15415415  -4.13413413  -4.11411411\n",
      "  -4.09409409  -4.07407407  -4.05405405  -4.03403403  -4.01401401\n",
      "  -3.99399399  -3.97397397  -3.95395395  -3.93393393  -3.91391391\n",
      "  -3.89389389  -3.87387387  -3.85385385  -3.83383383  -3.81381381\n",
      "  -3.79379379  -3.77377377  -3.75375375  -3.73373373  -3.71371371\n",
      "  -3.69369369  -3.67367367  -3.65365365  -3.63363363  -3.61361361\n",
      "  -3.59359359  -3.57357357  -3.55355355  -3.53353353  -3.51351351\n",
      "  -3.49349349  -3.47347347  -3.45345345  -3.43343343  -3.41341341\n",
      "  -3.39339339  -3.37337337  -3.35335335  -3.33333333  -3.31331331\n",
      "  -3.29329329  -3.27327327  -3.25325325  -3.23323323  -3.21321321\n",
      "  -3.19319319  -3.17317317  -3.15315315  -3.13313313  -3.11311311\n",
      "  -3.09309309  -3.07307307  -3.05305305  -3.03303303  -3.01301301\n",
      "  -2.99299299  -2.97297297  -2.95295295  -2.93293293  -2.91291291\n",
      "  -2.89289289  -2.87287287  -2.85285285  -2.83283283  -2.81281281\n",
      "  -2.79279279  -2.77277277  -2.75275275  -2.73273273  -2.71271271\n",
      "  -2.69269269  -2.67267267  -2.65265265  -2.63263263  -2.61261261\n",
      "  -2.59259259  -2.57257257  -2.55255255  -2.53253253  -2.51251251\n",
      "  -2.49249249  -2.47247247  -2.45245245  -2.43243243  -2.41241241\n",
      "  -2.39239239  -2.37237237  -2.35235235  -2.33233233  -2.31231231\n",
      "  -2.29229229  -2.27227227  -2.25225225  -2.23223223  -2.21221221\n",
      "  -2.19219219  -2.17217217  -2.15215215  -2.13213213  -2.11211211\n",
      "  -2.09209209  -2.07207207  -2.05205205  -2.03203203  -2.01201201\n",
      "  -1.99199199  -1.97197197  -1.95195195  -1.93193193  -1.91191191\n",
      "  -1.89189189  -1.87187187  -1.85185185  -1.83183183  -1.81181181\n",
      "  -1.79179179  -1.77177177  -1.75175175  -1.73173173  -1.71171171\n",
      "  -1.69169169  -1.67167167  -1.65165165  -1.63163163  -1.61161161\n",
      "  -1.59159159  -1.57157157  -1.55155155  -1.53153153  -1.51151151\n",
      "  -1.49149149  -1.47147147  -1.45145145  -1.43143143  -1.41141141\n",
      "  -1.39139139  -1.37137137  -1.35135135  -1.33133133  -1.31131131\n",
      "  -1.29129129  -1.27127127  -1.25125125  -1.23123123  -1.21121121\n",
      "  -1.19119119  -1.17117117  -1.15115115  -1.13113113  -1.11111111\n",
      "  -1.09109109  -1.07107107  -1.05105105  -1.03103103  -1.01101101\n",
      "  -0.99099099  -0.97097097  -0.95095095  -0.93093093  -0.91091091\n",
      "  -0.89089089  -0.87087087  -0.85085085  -0.83083083  -0.81081081\n",
      "  -0.79079079  -0.77077077  -0.75075075  -0.73073073  -0.71071071\n",
      "  -0.69069069  -0.67067067  -0.65065065  -0.63063063  -0.61061061\n",
      "  -0.59059059  -0.57057057  -0.55055055  -0.53053053  -0.51051051\n",
      "  -0.49049049  -0.47047047  -0.45045045  -0.43043043  -0.41041041\n",
      "  -0.39039039  -0.37037037  -0.35035035  -0.33033033  -0.31031031\n",
      "  -0.29029029  -0.27027027  -0.25025025  -0.23023023  -0.21021021\n",
      "  -0.19019019  -0.17017017  -0.15015015  -0.13013013  -0.11011011\n",
      "  -0.09009009  -0.07007007  -0.05005005  -0.03003003  -0.01001001\n",
      "   0.01001001   0.03003003   0.05005005   0.07007007   0.09009009\n",
      "   0.11011011   0.13013013   0.15015015   0.17017017   0.19019019\n",
      "   0.21021021   0.23023023   0.25025025   0.27027027   0.29029029\n",
      "   0.31031031   0.33033033   0.35035035   0.37037037   0.39039039\n",
      "   0.41041041   0.43043043   0.45045045   0.47047047   0.49049049\n",
      "   0.51051051   0.53053053   0.55055055   0.57057057   0.59059059\n",
      "   0.61061061   0.63063063   0.65065065   0.67067067   0.69069069\n",
      "   0.71071071   0.73073073   0.75075075   0.77077077   0.79079079\n",
      "   0.81081081   0.83083083   0.85085085   0.87087087   0.89089089\n",
      "   0.91091091   0.93093093   0.95095095   0.97097097   0.99099099\n",
      "   1.01101101   1.03103103   1.05105105   1.07107107   1.09109109\n",
      "   1.11111111   1.13113113   1.15115115   1.17117117   1.19119119\n",
      "   1.21121121   1.23123123   1.25125125   1.27127127   1.29129129\n",
      "   1.31131131   1.33133133   1.35135135   1.37137137   1.39139139\n",
      "   1.41141141   1.43143143   1.45145145   1.47147147   1.49149149\n",
      "   1.51151151   1.53153153   1.55155155   1.57157157   1.59159159\n",
      "   1.61161161   1.63163163   1.65165165   1.67167167   1.69169169\n",
      "   1.71171171   1.73173173   1.75175175   1.77177177   1.79179179\n",
      "   1.81181181   1.83183183   1.85185185   1.87187187   1.89189189\n",
      "   1.91191191   1.93193193   1.95195195   1.97197197   1.99199199\n",
      "   2.01201201   2.03203203   2.05205205   2.07207207   2.09209209\n",
      "   2.11211211   2.13213213   2.15215215   2.17217217   2.19219219\n",
      "   2.21221221   2.23223223   2.25225225   2.27227227   2.29229229\n",
      "   2.31231231   2.33233233   2.35235235   2.37237237   2.39239239\n",
      "   2.41241241   2.43243243   2.45245245   2.47247247   2.49249249\n",
      "   2.51251251   2.53253253   2.55255255   2.57257257   2.59259259\n",
      "   2.61261261   2.63263263   2.65265265   2.67267267   2.69269269\n",
      "   2.71271271   2.73273273   2.75275275   2.77277277   2.79279279\n",
      "   2.81281281   2.83283283   2.85285285   2.87287287   2.89289289\n",
      "   2.91291291   2.93293293   2.95295295   2.97297297   2.99299299\n",
      "   3.01301301   3.03303303   3.05305305   3.07307307   3.09309309\n",
      "   3.11311311   3.13313313   3.15315315   3.17317317   3.19319319\n",
      "   3.21321321   3.23323323   3.25325325   3.27327327   3.29329329\n",
      "   3.31331331   3.33333333   3.35335335   3.37337337   3.39339339\n",
      "   3.41341341   3.43343343   3.45345345   3.47347347   3.49349349\n",
      "   3.51351351   3.53353353   3.55355355   3.57357357   3.59359359\n",
      "   3.61361361   3.63363363   3.65365365   3.67367367   3.69369369\n",
      "   3.71371371   3.73373373   3.75375375   3.77377377   3.79379379\n",
      "   3.81381381   3.83383383   3.85385385   3.87387387   3.89389389\n",
      "   3.91391391   3.93393393   3.95395395   3.97397397   3.99399399\n",
      "   4.01401401   4.03403403   4.05405405   4.07407407   4.09409409\n",
      "   4.11411411   4.13413413   4.15415415   4.17417417   4.19419419\n",
      "   4.21421421   4.23423423   4.25425425   4.27427427   4.29429429\n",
      "   4.31431431   4.33433433   4.35435435   4.37437437   4.39439439\n",
      "   4.41441441   4.43443443   4.45445445   4.47447447   4.49449449\n",
      "   4.51451451   4.53453453   4.55455455   4.57457457   4.59459459\n",
      "   4.61461461   4.63463463   4.65465465   4.67467467   4.69469469\n",
      "   4.71471471   4.73473473   4.75475475   4.77477477   4.79479479\n",
      "   4.81481481   4.83483483   4.85485485   4.87487487   4.89489489\n",
      "   4.91491491   4.93493493   4.95495495   4.97497497   4.99499499\n",
      "   5.01501502   5.03503504   5.05505506   5.07507508   5.0950951\n",
      "   5.11511512   5.13513514   5.15515516   5.17517518   5.1951952\n",
      "   5.21521522   5.23523524   5.25525526   5.27527528   5.2952953\n",
      "   5.31531532   5.33533534   5.35535536   5.37537538   5.3953954\n",
      "   5.41541542   5.43543544   5.45545546   5.47547548   5.4954955\n",
      "   5.51551552   5.53553554   5.55555556   5.57557558   5.5955956\n",
      "   5.61561562   5.63563564   5.65565566   5.67567568   5.6956957\n",
      "   5.71571572   5.73573574   5.75575576   5.77577578   5.7957958\n",
      "   5.81581582   5.83583584   5.85585586   5.87587588   5.8958959\n",
      "   5.91591592   5.93593594   5.95595596   5.97597598   5.995996\n",
      "   6.01601602   6.03603604   6.05605606   6.07607608   6.0960961\n",
      "   6.11611612   6.13613614   6.15615616   6.17617618   6.1961962\n",
      "   6.21621622   6.23623624   6.25625626   6.27627628   6.2962963\n",
      "   6.31631632   6.33633634   6.35635636   6.37637638   6.3963964\n",
      "   6.41641642   6.43643644   6.45645646   6.47647648   6.4964965\n",
      "   6.51651652   6.53653654   6.55655656   6.57657658   6.5965966\n",
      "   6.61661662   6.63663664   6.65665666   6.67667668   6.6966967\n",
      "   6.71671672   6.73673674   6.75675676   6.77677678   6.7967968\n",
      "   6.81681682   6.83683684   6.85685686   6.87687688   6.8968969\n",
      "   6.91691692   6.93693694   6.95695696   6.97697698   6.996997\n",
      "   7.01701702   7.03703704   7.05705706   7.07707708   7.0970971\n",
      "   7.11711712   7.13713714   7.15715716   7.17717718   7.1971972\n",
      "   7.21721722   7.23723724   7.25725726   7.27727728   7.2972973\n",
      "   7.31731732   7.33733734   7.35735736   7.37737738   7.3973974\n",
      "   7.41741742   7.43743744   7.45745746   7.47747748   7.4974975\n",
      "   7.51751752   7.53753754   7.55755756   7.57757758   7.5975976\n",
      "   7.61761762   7.63763764   7.65765766   7.67767768   7.6976977\n",
      "   7.71771772   7.73773774   7.75775776   7.77777778   7.7977978\n",
      "   7.81781782   7.83783784   7.85785786   7.87787788   7.8978979\n",
      "   7.91791792   7.93793794   7.95795796   7.97797798   7.997998\n",
      "   8.01801802   8.03803804   8.05805806   8.07807808   8.0980981\n",
      "   8.11811812   8.13813814   8.15815816   8.17817818   8.1981982\n",
      "   8.21821822   8.23823824   8.25825826   8.27827828   8.2982983\n",
      "   8.31831832   8.33833834   8.35835836   8.37837838   8.3983984\n",
      "   8.41841842   8.43843844   8.45845846   8.47847848   8.4984985\n",
      "   8.51851852   8.53853854   8.55855856   8.57857858   8.5985986\n",
      "   8.61861862   8.63863864   8.65865866   8.67867868   8.6986987\n",
      "   8.71871872   8.73873874   8.75875876   8.77877878   8.7987988\n",
      "   8.81881882   8.83883884   8.85885886   8.87887888   8.8988989\n",
      "   8.91891892   8.93893894   8.95895896   8.97897898   8.998999\n",
      "   9.01901902   9.03903904   9.05905906   9.07907908   9.0990991\n",
      "   9.11911912   9.13913914   9.15915916   9.17917918   9.1991992\n",
      "   9.21921922   9.23923924   9.25925926   9.27927928   9.2992993\n",
      "   9.31931932   9.33933934   9.35935936   9.37937938   9.3993994\n",
      "   9.41941942   9.43943944   9.45945946   9.47947948   9.4994995\n",
      "   9.51951952   9.53953954   9.55955956   9.57957958   9.5995996\n",
      "   9.61961962   9.63963964   9.65965966   9.67967968   9.6996997\n",
      "   9.71971972   9.73973974   9.75975976   9.77977978   9.7997998\n",
      "   9.81981982   9.83983984   9.85985986   9.87987988   9.8998999\n",
      "   9.91991992   9.93993994   9.95995996   9.97997998  10.        ]\n",
      "[[-10.        ]\n",
      " [ -9.97997998]\n",
      " [ -9.95995996]\n",
      " [ -9.93993994]\n",
      " [ -9.91991992]\n",
      " [ -9.8998999 ]\n",
      " [ -9.87987988]\n",
      " [ -9.85985986]\n",
      " [ -9.83983984]\n",
      " [ -9.81981982]\n",
      " [ -9.7997998 ]\n",
      " [ -9.77977978]\n",
      " [ -9.75975976]\n",
      " [ -9.73973974]\n",
      " [ -9.71971972]\n",
      " [ -9.6996997 ]\n",
      " [ -9.67967968]\n",
      " [ -9.65965966]\n",
      " [ -9.63963964]\n",
      " [ -9.61961962]\n",
      " [ -9.5995996 ]\n",
      " [ -9.57957958]\n",
      " [ -9.55955956]\n",
      " [ -9.53953954]\n",
      " [ -9.51951952]\n",
      " [ -9.4994995 ]\n",
      " [ -9.47947948]\n",
      " [ -9.45945946]\n",
      " [ -9.43943944]\n",
      " [ -9.41941942]\n",
      " [ -9.3993994 ]\n",
      " [ -9.37937938]\n",
      " [ -9.35935936]\n",
      " [ -9.33933934]\n",
      " [ -9.31931932]\n",
      " [ -9.2992993 ]\n",
      " [ -9.27927928]\n",
      " [ -9.25925926]\n",
      " [ -9.23923924]\n",
      " [ -9.21921922]\n",
      " [ -9.1991992 ]\n",
      " [ -9.17917918]\n",
      " [ -9.15915916]\n",
      " [ -9.13913914]\n",
      " [ -9.11911912]\n",
      " [ -9.0990991 ]\n",
      " [ -9.07907908]\n",
      " [ -9.05905906]\n",
      " [ -9.03903904]\n",
      " [ -9.01901902]\n",
      " [ -8.998999  ]\n",
      " [ -8.97897898]\n",
      " [ -8.95895896]\n",
      " [ -8.93893894]\n",
      " [ -8.91891892]\n",
      " [ -8.8988989 ]\n",
      " [ -8.87887888]\n",
      " [ -8.85885886]\n",
      " [ -8.83883884]\n",
      " [ -8.81881882]\n",
      " [ -8.7987988 ]\n",
      " [ -8.77877878]\n",
      " [ -8.75875876]\n",
      " [ -8.73873874]\n",
      " [ -8.71871872]\n",
      " [ -8.6986987 ]\n",
      " [ -8.67867868]\n",
      " [ -8.65865866]\n",
      " [ -8.63863864]\n",
      " [ -8.61861862]\n",
      " [ -8.5985986 ]\n",
      " [ -8.57857858]\n",
      " [ -8.55855856]\n",
      " [ -8.53853854]\n",
      " [ -8.51851852]\n",
      " [ -8.4984985 ]\n",
      " [ -8.47847848]\n",
      " [ -8.45845846]\n",
      " [ -8.43843844]\n",
      " [ -8.41841842]\n",
      " [ -8.3983984 ]\n",
      " [ -8.37837838]\n",
      " [ -8.35835836]\n",
      " [ -8.33833834]\n",
      " [ -8.31831832]\n",
      " [ -8.2982983 ]\n",
      " [ -8.27827828]\n",
      " [ -8.25825826]\n",
      " [ -8.23823824]\n",
      " [ -8.21821822]\n",
      " [ -8.1981982 ]\n",
      " [ -8.17817818]\n",
      " [ -8.15815816]\n",
      " [ -8.13813814]\n",
      " [ -8.11811812]\n",
      " [ -8.0980981 ]\n",
      " [ -8.07807808]\n",
      " [ -8.05805806]\n",
      " [ -8.03803804]\n",
      " [ -8.01801802]\n",
      " [ -7.997998  ]\n",
      " [ -7.97797798]\n",
      " [ -7.95795796]\n",
      " [ -7.93793794]\n",
      " [ -7.91791792]\n",
      " [ -7.8978979 ]\n",
      " [ -7.87787788]\n",
      " [ -7.85785786]\n",
      " [ -7.83783784]\n",
      " [ -7.81781782]\n",
      " [ -7.7977978 ]\n",
      " [ -7.77777778]\n",
      " [ -7.75775776]\n",
      " [ -7.73773774]\n",
      " [ -7.71771772]\n",
      " [ -7.6976977 ]\n",
      " [ -7.67767768]\n",
      " [ -7.65765766]\n",
      " [ -7.63763764]\n",
      " [ -7.61761762]\n",
      " [ -7.5975976 ]\n",
      " [ -7.57757758]\n",
      " [ -7.55755756]\n",
      " [ -7.53753754]\n",
      " [ -7.51751752]\n",
      " [ -7.4974975 ]\n",
      " [ -7.47747748]\n",
      " [ -7.45745746]\n",
      " [ -7.43743744]\n",
      " [ -7.41741742]\n",
      " [ -7.3973974 ]\n",
      " [ -7.37737738]\n",
      " [ -7.35735736]\n",
      " [ -7.33733734]\n",
      " [ -7.31731732]\n",
      " [ -7.2972973 ]\n",
      " [ -7.27727728]\n",
      " [ -7.25725726]\n",
      " [ -7.23723724]\n",
      " [ -7.21721722]\n",
      " [ -7.1971972 ]\n",
      " [ -7.17717718]\n",
      " [ -7.15715716]\n",
      " [ -7.13713714]\n",
      " [ -7.11711712]\n",
      " [ -7.0970971 ]\n",
      " [ -7.07707708]\n",
      " [ -7.05705706]\n",
      " [ -7.03703704]\n",
      " [ -7.01701702]\n",
      " [ -6.996997  ]\n",
      " [ -6.97697698]\n",
      " [ -6.95695696]\n",
      " [ -6.93693694]\n",
      " [ -6.91691692]\n",
      " [ -6.8968969 ]\n",
      " [ -6.87687688]\n",
      " [ -6.85685686]\n",
      " [ -6.83683684]\n",
      " [ -6.81681682]\n",
      " [ -6.7967968 ]\n",
      " [ -6.77677678]\n",
      " [ -6.75675676]\n",
      " [ -6.73673674]\n",
      " [ -6.71671672]\n",
      " [ -6.6966967 ]\n",
      " [ -6.67667668]\n",
      " [ -6.65665666]\n",
      " [ -6.63663664]\n",
      " [ -6.61661662]\n",
      " [ -6.5965966 ]\n",
      " [ -6.57657658]\n",
      " [ -6.55655656]\n",
      " [ -6.53653654]\n",
      " [ -6.51651652]\n",
      " [ -6.4964965 ]\n",
      " [ -6.47647648]\n",
      " [ -6.45645646]\n",
      " [ -6.43643644]\n",
      " [ -6.41641642]\n",
      " [ -6.3963964 ]\n",
      " [ -6.37637638]\n",
      " [ -6.35635636]\n",
      " [ -6.33633634]\n",
      " [ -6.31631632]\n",
      " [ -6.2962963 ]\n",
      " [ -6.27627628]\n",
      " [ -6.25625626]\n",
      " [ -6.23623624]\n",
      " [ -6.21621622]\n",
      " [ -6.1961962 ]\n",
      " [ -6.17617618]\n",
      " [ -6.15615616]\n",
      " [ -6.13613614]\n",
      " [ -6.11611612]\n",
      " [ -6.0960961 ]\n",
      " [ -6.07607608]\n",
      " [ -6.05605606]\n",
      " [ -6.03603604]\n",
      " [ -6.01601602]\n",
      " [ -5.995996  ]\n",
      " [ -5.97597598]\n",
      " [ -5.95595596]\n",
      " [ -5.93593594]\n",
      " [ -5.91591592]\n",
      " [ -5.8958959 ]\n",
      " [ -5.87587588]\n",
      " [ -5.85585586]\n",
      " [ -5.83583584]\n",
      " [ -5.81581582]\n",
      " [ -5.7957958 ]\n",
      " [ -5.77577578]\n",
      " [ -5.75575576]\n",
      " [ -5.73573574]\n",
      " [ -5.71571572]\n",
      " [ -5.6956957 ]\n",
      " [ -5.67567568]\n",
      " [ -5.65565566]\n",
      " [ -5.63563564]\n",
      " [ -5.61561562]\n",
      " [ -5.5955956 ]\n",
      " [ -5.57557558]\n",
      " [ -5.55555556]\n",
      " [ -5.53553554]\n",
      " [ -5.51551552]\n",
      " [ -5.4954955 ]\n",
      " [ -5.47547548]\n",
      " [ -5.45545546]\n",
      " [ -5.43543544]\n",
      " [ -5.41541542]\n",
      " [ -5.3953954 ]\n",
      " [ -5.37537538]\n",
      " [ -5.35535536]\n",
      " [ -5.33533534]\n",
      " [ -5.31531532]\n",
      " [ -5.2952953 ]\n",
      " [ -5.27527528]\n",
      " [ -5.25525526]\n",
      " [ -5.23523524]\n",
      " [ -5.21521522]\n",
      " [ -5.1951952 ]\n",
      " [ -5.17517518]\n",
      " [ -5.15515516]\n",
      " [ -5.13513514]\n",
      " [ -5.11511512]\n",
      " [ -5.0950951 ]\n",
      " [ -5.07507508]\n",
      " [ -5.05505506]\n",
      " [ -5.03503504]\n",
      " [ -5.01501502]\n",
      " [ -4.99499499]\n",
      " [ -4.97497497]\n",
      " [ -4.95495495]\n",
      " [ -4.93493493]\n",
      " [ -4.91491491]\n",
      " [ -4.89489489]\n",
      " [ -4.87487487]\n",
      " [ -4.85485485]\n",
      " [ -4.83483483]\n",
      " [ -4.81481481]\n",
      " [ -4.79479479]\n",
      " [ -4.77477477]\n",
      " [ -4.75475475]\n",
      " [ -4.73473473]\n",
      " [ -4.71471471]\n",
      " [ -4.69469469]\n",
      " [ -4.67467467]\n",
      " [ -4.65465465]\n",
      " [ -4.63463463]\n",
      " [ -4.61461461]\n",
      " [ -4.59459459]\n",
      " [ -4.57457457]\n",
      " [ -4.55455455]\n",
      " [ -4.53453453]\n",
      " [ -4.51451451]\n",
      " [ -4.49449449]\n",
      " [ -4.47447447]\n",
      " [ -4.45445445]\n",
      " [ -4.43443443]\n",
      " [ -4.41441441]\n",
      " [ -4.39439439]\n",
      " [ -4.37437437]\n",
      " [ -4.35435435]\n",
      " [ -4.33433433]\n",
      " [ -4.31431431]\n",
      " [ -4.29429429]\n",
      " [ -4.27427427]\n",
      " [ -4.25425425]\n",
      " [ -4.23423423]\n",
      " [ -4.21421421]\n",
      " [ -4.19419419]\n",
      " [ -4.17417417]\n",
      " [ -4.15415415]\n",
      " [ -4.13413413]\n",
      " [ -4.11411411]\n",
      " [ -4.09409409]\n",
      " [ -4.07407407]\n",
      " [ -4.05405405]\n",
      " [ -4.03403403]\n",
      " [ -4.01401401]\n",
      " [ -3.99399399]\n",
      " [ -3.97397397]\n",
      " [ -3.95395395]\n",
      " [ -3.93393393]\n",
      " [ -3.91391391]\n",
      " [ -3.89389389]\n",
      " [ -3.87387387]\n",
      " [ -3.85385385]\n",
      " [ -3.83383383]\n",
      " [ -3.81381381]\n",
      " [ -3.79379379]\n",
      " [ -3.77377377]\n",
      " [ -3.75375375]\n",
      " [ -3.73373373]\n",
      " [ -3.71371371]\n",
      " [ -3.69369369]\n",
      " [ -3.67367367]\n",
      " [ -3.65365365]\n",
      " [ -3.63363363]\n",
      " [ -3.61361361]\n",
      " [ -3.59359359]\n",
      " [ -3.57357357]\n",
      " [ -3.55355355]\n",
      " [ -3.53353353]\n",
      " [ -3.51351351]\n",
      " [ -3.49349349]\n",
      " [ -3.47347347]\n",
      " [ -3.45345345]\n",
      " [ -3.43343343]\n",
      " [ -3.41341341]\n",
      " [ -3.39339339]\n",
      " [ -3.37337337]\n",
      " [ -3.35335335]\n",
      " [ -3.33333333]\n",
      " [ -3.31331331]\n",
      " [ -3.29329329]\n",
      " [ -3.27327327]\n",
      " [ -3.25325325]\n",
      " [ -3.23323323]\n",
      " [ -3.21321321]\n",
      " [ -3.19319319]\n",
      " [ -3.17317317]\n",
      " [ -3.15315315]\n",
      " [ -3.13313313]\n",
      " [ -3.11311311]\n",
      " [ -3.09309309]\n",
      " [ -3.07307307]\n",
      " [ -3.05305305]\n",
      " [ -3.03303303]\n",
      " [ -3.01301301]\n",
      " [ -2.99299299]\n",
      " [ -2.97297297]\n",
      " [ -2.95295295]\n",
      " [ -2.93293293]\n",
      " [ -2.91291291]\n",
      " [ -2.89289289]\n",
      " [ -2.87287287]\n",
      " [ -2.85285285]\n",
      " [ -2.83283283]\n",
      " [ -2.81281281]\n",
      " [ -2.79279279]\n",
      " [ -2.77277277]\n",
      " [ -2.75275275]\n",
      " [ -2.73273273]\n",
      " [ -2.71271271]\n",
      " [ -2.69269269]\n",
      " [ -2.67267267]\n",
      " [ -2.65265265]\n",
      " [ -2.63263263]\n",
      " [ -2.61261261]\n",
      " [ -2.59259259]\n",
      " [ -2.57257257]\n",
      " [ -2.55255255]\n",
      " [ -2.53253253]\n",
      " [ -2.51251251]\n",
      " [ -2.49249249]\n",
      " [ -2.47247247]\n",
      " [ -2.45245245]\n",
      " [ -2.43243243]\n",
      " [ -2.41241241]\n",
      " [ -2.39239239]\n",
      " [ -2.37237237]\n",
      " [ -2.35235235]\n",
      " [ -2.33233233]\n",
      " [ -2.31231231]\n",
      " [ -2.29229229]\n",
      " [ -2.27227227]\n",
      " [ -2.25225225]\n",
      " [ -2.23223223]\n",
      " [ -2.21221221]\n",
      " [ -2.19219219]\n",
      " [ -2.17217217]\n",
      " [ -2.15215215]\n",
      " [ -2.13213213]\n",
      " [ -2.11211211]\n",
      " [ -2.09209209]\n",
      " [ -2.07207207]\n",
      " [ -2.05205205]\n",
      " [ -2.03203203]\n",
      " [ -2.01201201]\n",
      " [ -1.99199199]\n",
      " [ -1.97197197]\n",
      " [ -1.95195195]\n",
      " [ -1.93193193]\n",
      " [ -1.91191191]\n",
      " [ -1.89189189]\n",
      " [ -1.87187187]\n",
      " [ -1.85185185]\n",
      " [ -1.83183183]\n",
      " [ -1.81181181]\n",
      " [ -1.79179179]\n",
      " [ -1.77177177]\n",
      " [ -1.75175175]\n",
      " [ -1.73173173]\n",
      " [ -1.71171171]\n",
      " [ -1.69169169]\n",
      " [ -1.67167167]\n",
      " [ -1.65165165]\n",
      " [ -1.63163163]\n",
      " [ -1.61161161]\n",
      " [ -1.59159159]\n",
      " [ -1.57157157]\n",
      " [ -1.55155155]\n",
      " [ -1.53153153]\n",
      " [ -1.51151151]\n",
      " [ -1.49149149]\n",
      " [ -1.47147147]\n",
      " [ -1.45145145]\n",
      " [ -1.43143143]\n",
      " [ -1.41141141]\n",
      " [ -1.39139139]\n",
      " [ -1.37137137]\n",
      " [ -1.35135135]\n",
      " [ -1.33133133]\n",
      " [ -1.31131131]\n",
      " [ -1.29129129]\n",
      " [ -1.27127127]\n",
      " [ -1.25125125]\n",
      " [ -1.23123123]\n",
      " [ -1.21121121]\n",
      " [ -1.19119119]\n",
      " [ -1.17117117]\n",
      " [ -1.15115115]\n",
      " [ -1.13113113]\n",
      " [ -1.11111111]\n",
      " [ -1.09109109]\n",
      " [ -1.07107107]\n",
      " [ -1.05105105]\n",
      " [ -1.03103103]\n",
      " [ -1.01101101]\n",
      " [ -0.99099099]\n",
      " [ -0.97097097]\n",
      " [ -0.95095095]\n",
      " [ -0.93093093]\n",
      " [ -0.91091091]\n",
      " [ -0.89089089]\n",
      " [ -0.87087087]\n",
      " [ -0.85085085]\n",
      " [ -0.83083083]\n",
      " [ -0.81081081]\n",
      " [ -0.79079079]\n",
      " [ -0.77077077]\n",
      " [ -0.75075075]\n",
      " [ -0.73073073]\n",
      " [ -0.71071071]\n",
      " [ -0.69069069]\n",
      " [ -0.67067067]\n",
      " [ -0.65065065]\n",
      " [ -0.63063063]\n",
      " [ -0.61061061]\n",
      " [ -0.59059059]\n",
      " [ -0.57057057]\n",
      " [ -0.55055055]\n",
      " [ -0.53053053]\n",
      " [ -0.51051051]\n",
      " [ -0.49049049]\n",
      " [ -0.47047047]\n",
      " [ -0.45045045]\n",
      " [ -0.43043043]\n",
      " [ -0.41041041]\n",
      " [ -0.39039039]\n",
      " [ -0.37037037]\n",
      " [ -0.35035035]\n",
      " [ -0.33033033]\n",
      " [ -0.31031031]\n",
      " [ -0.29029029]\n",
      " [ -0.27027027]\n",
      " [ -0.25025025]\n",
      " [ -0.23023023]\n",
      " [ -0.21021021]\n",
      " [ -0.19019019]\n",
      " [ -0.17017017]\n",
      " [ -0.15015015]\n",
      " [ -0.13013013]\n",
      " [ -0.11011011]\n",
      " [ -0.09009009]\n",
      " [ -0.07007007]\n",
      " [ -0.05005005]\n",
      " [ -0.03003003]\n",
      " [ -0.01001001]\n",
      " [  0.01001001]\n",
      " [  0.03003003]\n",
      " [  0.05005005]\n",
      " [  0.07007007]\n",
      " [  0.09009009]\n",
      " [  0.11011011]\n",
      " [  0.13013013]\n",
      " [  0.15015015]\n",
      " [  0.17017017]\n",
      " [  0.19019019]\n",
      " [  0.21021021]\n",
      " [  0.23023023]\n",
      " [  0.25025025]\n",
      " [  0.27027027]\n",
      " [  0.29029029]\n",
      " [  0.31031031]\n",
      " [  0.33033033]\n",
      " [  0.35035035]\n",
      " [  0.37037037]\n",
      " [  0.39039039]\n",
      " [  0.41041041]\n",
      " [  0.43043043]\n",
      " [  0.45045045]\n",
      " [  0.47047047]\n",
      " [  0.49049049]\n",
      " [  0.51051051]\n",
      " [  0.53053053]\n",
      " [  0.55055055]\n",
      " [  0.57057057]\n",
      " [  0.59059059]\n",
      " [  0.61061061]\n",
      " [  0.63063063]\n",
      " [  0.65065065]\n",
      " [  0.67067067]\n",
      " [  0.69069069]\n",
      " [  0.71071071]\n",
      " [  0.73073073]\n",
      " [  0.75075075]\n",
      " [  0.77077077]\n",
      " [  0.79079079]\n",
      " [  0.81081081]\n",
      " [  0.83083083]\n",
      " [  0.85085085]\n",
      " [  0.87087087]\n",
      " [  0.89089089]\n",
      " [  0.91091091]\n",
      " [  0.93093093]\n",
      " [  0.95095095]\n",
      " [  0.97097097]\n",
      " [  0.99099099]\n",
      " [  1.01101101]\n",
      " [  1.03103103]\n",
      " [  1.05105105]\n",
      " [  1.07107107]\n",
      " [  1.09109109]\n",
      " [  1.11111111]\n",
      " [  1.13113113]\n",
      " [  1.15115115]\n",
      " [  1.17117117]\n",
      " [  1.19119119]\n",
      " [  1.21121121]\n",
      " [  1.23123123]\n",
      " [  1.25125125]\n",
      " [  1.27127127]\n",
      " [  1.29129129]\n",
      " [  1.31131131]\n",
      " [  1.33133133]\n",
      " [  1.35135135]\n",
      " [  1.37137137]\n",
      " [  1.39139139]\n",
      " [  1.41141141]\n",
      " [  1.43143143]\n",
      " [  1.45145145]\n",
      " [  1.47147147]\n",
      " [  1.49149149]\n",
      " [  1.51151151]\n",
      " [  1.53153153]\n",
      " [  1.55155155]\n",
      " [  1.57157157]\n",
      " [  1.59159159]\n",
      " [  1.61161161]\n",
      " [  1.63163163]\n",
      " [  1.65165165]\n",
      " [  1.67167167]\n",
      " [  1.69169169]\n",
      " [  1.71171171]\n",
      " [  1.73173173]\n",
      " [  1.75175175]\n",
      " [  1.77177177]\n",
      " [  1.79179179]\n",
      " [  1.81181181]\n",
      " [  1.83183183]\n",
      " [  1.85185185]\n",
      " [  1.87187187]\n",
      " [  1.89189189]\n",
      " [  1.91191191]\n",
      " [  1.93193193]\n",
      " [  1.95195195]\n",
      " [  1.97197197]\n",
      " [  1.99199199]\n",
      " [  2.01201201]\n",
      " [  2.03203203]\n",
      " [  2.05205205]\n",
      " [  2.07207207]\n",
      " [  2.09209209]\n",
      " [  2.11211211]\n",
      " [  2.13213213]\n",
      " [  2.15215215]\n",
      " [  2.17217217]\n",
      " [  2.19219219]\n",
      " [  2.21221221]\n",
      " [  2.23223223]\n",
      " [  2.25225225]\n",
      " [  2.27227227]\n",
      " [  2.29229229]\n",
      " [  2.31231231]\n",
      " [  2.33233233]\n",
      " [  2.35235235]\n",
      " [  2.37237237]\n",
      " [  2.39239239]\n",
      " [  2.41241241]\n",
      " [  2.43243243]\n",
      " [  2.45245245]\n",
      " [  2.47247247]\n",
      " [  2.49249249]\n",
      " [  2.51251251]\n",
      " [  2.53253253]\n",
      " [  2.55255255]\n",
      " [  2.57257257]\n",
      " [  2.59259259]\n",
      " [  2.61261261]\n",
      " [  2.63263263]\n",
      " [  2.65265265]\n",
      " [  2.67267267]\n",
      " [  2.69269269]\n",
      " [  2.71271271]\n",
      " [  2.73273273]\n",
      " [  2.75275275]\n",
      " [  2.77277277]\n",
      " [  2.79279279]\n",
      " [  2.81281281]\n",
      " [  2.83283283]\n",
      " [  2.85285285]\n",
      " [  2.87287287]\n",
      " [  2.89289289]\n",
      " [  2.91291291]\n",
      " [  2.93293293]\n",
      " [  2.95295295]\n",
      " [  2.97297297]\n",
      " [  2.99299299]\n",
      " [  3.01301301]\n",
      " [  3.03303303]\n",
      " [  3.05305305]\n",
      " [  3.07307307]\n",
      " [  3.09309309]\n",
      " [  3.11311311]\n",
      " [  3.13313313]\n",
      " [  3.15315315]\n",
      " [  3.17317317]\n",
      " [  3.19319319]\n",
      " [  3.21321321]\n",
      " [  3.23323323]\n",
      " [  3.25325325]\n",
      " [  3.27327327]\n",
      " [  3.29329329]\n",
      " [  3.31331331]\n",
      " [  3.33333333]\n",
      " [  3.35335335]\n",
      " [  3.37337337]\n",
      " [  3.39339339]\n",
      " [  3.41341341]\n",
      " [  3.43343343]\n",
      " [  3.45345345]\n",
      " [  3.47347347]\n",
      " [  3.49349349]\n",
      " [  3.51351351]\n",
      " [  3.53353353]\n",
      " [  3.55355355]\n",
      " [  3.57357357]\n",
      " [  3.59359359]\n",
      " [  3.61361361]\n",
      " [  3.63363363]\n",
      " [  3.65365365]\n",
      " [  3.67367367]\n",
      " [  3.69369369]\n",
      " [  3.71371371]\n",
      " [  3.73373373]\n",
      " [  3.75375375]\n",
      " [  3.77377377]\n",
      " [  3.79379379]\n",
      " [  3.81381381]\n",
      " [  3.83383383]\n",
      " [  3.85385385]\n",
      " [  3.87387387]\n",
      " [  3.89389389]\n",
      " [  3.91391391]\n",
      " [  3.93393393]\n",
      " [  3.95395395]\n",
      " [  3.97397397]\n",
      " [  3.99399399]\n",
      " [  4.01401401]\n",
      " [  4.03403403]\n",
      " [  4.05405405]\n",
      " [  4.07407407]\n",
      " [  4.09409409]\n",
      " [  4.11411411]\n",
      " [  4.13413413]\n",
      " [  4.15415415]\n",
      " [  4.17417417]\n",
      " [  4.19419419]\n",
      " [  4.21421421]\n",
      " [  4.23423423]\n",
      " [  4.25425425]\n",
      " [  4.27427427]\n",
      " [  4.29429429]\n",
      " [  4.31431431]\n",
      " [  4.33433433]\n",
      " [  4.35435435]\n",
      " [  4.37437437]\n",
      " [  4.39439439]\n",
      " [  4.41441441]\n",
      " [  4.43443443]\n",
      " [  4.45445445]\n",
      " [  4.47447447]\n",
      " [  4.49449449]\n",
      " [  4.51451451]\n",
      " [  4.53453453]\n",
      " [  4.55455455]\n",
      " [  4.57457457]\n",
      " [  4.59459459]\n",
      " [  4.61461461]\n",
      " [  4.63463463]\n",
      " [  4.65465465]\n",
      " [  4.67467467]\n",
      " [  4.69469469]\n",
      " [  4.71471471]\n",
      " [  4.73473473]\n",
      " [  4.75475475]\n",
      " [  4.77477477]\n",
      " [  4.79479479]\n",
      " [  4.81481481]\n",
      " [  4.83483483]\n",
      " [  4.85485485]\n",
      " [  4.87487487]\n",
      " [  4.89489489]\n",
      " [  4.91491491]\n",
      " [  4.93493493]\n",
      " [  4.95495495]\n",
      " [  4.97497497]\n",
      " [  4.99499499]\n",
      " [  5.01501502]\n",
      " [  5.03503504]\n",
      " [  5.05505506]\n",
      " [  5.07507508]\n",
      " [  5.0950951 ]\n",
      " [  5.11511512]\n",
      " [  5.13513514]\n",
      " [  5.15515516]\n",
      " [  5.17517518]\n",
      " [  5.1951952 ]\n",
      " [  5.21521522]\n",
      " [  5.23523524]\n",
      " [  5.25525526]\n",
      " [  5.27527528]\n",
      " [  5.2952953 ]\n",
      " [  5.31531532]\n",
      " [  5.33533534]\n",
      " [  5.35535536]\n",
      " [  5.37537538]\n",
      " [  5.3953954 ]\n",
      " [  5.41541542]\n",
      " [  5.43543544]\n",
      " [  5.45545546]\n",
      " [  5.47547548]\n",
      " [  5.4954955 ]\n",
      " [  5.51551552]\n",
      " [  5.53553554]\n",
      " [  5.55555556]\n",
      " [  5.57557558]\n",
      " [  5.5955956 ]\n",
      " [  5.61561562]\n",
      " [  5.63563564]\n",
      " [  5.65565566]\n",
      " [  5.67567568]\n",
      " [  5.6956957 ]\n",
      " [  5.71571572]\n",
      " [  5.73573574]\n",
      " [  5.75575576]\n",
      " [  5.77577578]\n",
      " [  5.7957958 ]\n",
      " [  5.81581582]\n",
      " [  5.83583584]\n",
      " [  5.85585586]\n",
      " [  5.87587588]\n",
      " [  5.8958959 ]\n",
      " [  5.91591592]\n",
      " [  5.93593594]\n",
      " [  5.95595596]\n",
      " [  5.97597598]\n",
      " [  5.995996  ]\n",
      " [  6.01601602]\n",
      " [  6.03603604]\n",
      " [  6.05605606]\n",
      " [  6.07607608]\n",
      " [  6.0960961 ]\n",
      " [  6.11611612]\n",
      " [  6.13613614]\n",
      " [  6.15615616]\n",
      " [  6.17617618]\n",
      " [  6.1961962 ]\n",
      " [  6.21621622]\n",
      " [  6.23623624]\n",
      " [  6.25625626]\n",
      " [  6.27627628]\n",
      " [  6.2962963 ]\n",
      " [  6.31631632]\n",
      " [  6.33633634]\n",
      " [  6.35635636]\n",
      " [  6.37637638]\n",
      " [  6.3963964 ]\n",
      " [  6.41641642]\n",
      " [  6.43643644]\n",
      " [  6.45645646]\n",
      " [  6.47647648]\n",
      " [  6.4964965 ]\n",
      " [  6.51651652]\n",
      " [  6.53653654]\n",
      " [  6.55655656]\n",
      " [  6.57657658]\n",
      " [  6.5965966 ]\n",
      " [  6.61661662]\n",
      " [  6.63663664]\n",
      " [  6.65665666]\n",
      " [  6.67667668]\n",
      " [  6.6966967 ]\n",
      " [  6.71671672]\n",
      " [  6.73673674]\n",
      " [  6.75675676]\n",
      " [  6.77677678]\n",
      " [  6.7967968 ]\n",
      " [  6.81681682]\n",
      " [  6.83683684]\n",
      " [  6.85685686]\n",
      " [  6.87687688]\n",
      " [  6.8968969 ]\n",
      " [  6.91691692]\n",
      " [  6.93693694]\n",
      " [  6.95695696]\n",
      " [  6.97697698]\n",
      " [  6.996997  ]\n",
      " [  7.01701702]\n",
      " [  7.03703704]\n",
      " [  7.05705706]\n",
      " [  7.07707708]\n",
      " [  7.0970971 ]\n",
      " [  7.11711712]\n",
      " [  7.13713714]\n",
      " [  7.15715716]\n",
      " [  7.17717718]\n",
      " [  7.1971972 ]\n",
      " [  7.21721722]\n",
      " [  7.23723724]\n",
      " [  7.25725726]\n",
      " [  7.27727728]\n",
      " [  7.2972973 ]\n",
      " [  7.31731732]\n",
      " [  7.33733734]\n",
      " [  7.35735736]\n",
      " [  7.37737738]\n",
      " [  7.3973974 ]\n",
      " [  7.41741742]\n",
      " [  7.43743744]\n",
      " [  7.45745746]\n",
      " [  7.47747748]\n",
      " [  7.4974975 ]\n",
      " [  7.51751752]\n",
      " [  7.53753754]\n",
      " [  7.55755756]\n",
      " [  7.57757758]\n",
      " [  7.5975976 ]\n",
      " [  7.61761762]\n",
      " [  7.63763764]\n",
      " [  7.65765766]\n",
      " [  7.67767768]\n",
      " [  7.6976977 ]\n",
      " [  7.71771772]\n",
      " [  7.73773774]\n",
      " [  7.75775776]\n",
      " [  7.77777778]\n",
      " [  7.7977978 ]\n",
      " [  7.81781782]\n",
      " [  7.83783784]\n",
      " [  7.85785786]\n",
      " [  7.87787788]\n",
      " [  7.8978979 ]\n",
      " [  7.91791792]\n",
      " [  7.93793794]\n",
      " [  7.95795796]\n",
      " [  7.97797798]\n",
      " [  7.997998  ]\n",
      " [  8.01801802]\n",
      " [  8.03803804]\n",
      " [  8.05805806]\n",
      " [  8.07807808]\n",
      " [  8.0980981 ]\n",
      " [  8.11811812]\n",
      " [  8.13813814]\n",
      " [  8.15815816]\n",
      " [  8.17817818]\n",
      " [  8.1981982 ]\n",
      " [  8.21821822]\n",
      " [  8.23823824]\n",
      " [  8.25825826]\n",
      " [  8.27827828]\n",
      " [  8.2982983 ]\n",
      " [  8.31831832]\n",
      " [  8.33833834]\n",
      " [  8.35835836]\n",
      " [  8.37837838]\n",
      " [  8.3983984 ]\n",
      " [  8.41841842]\n",
      " [  8.43843844]\n",
      " [  8.45845846]\n",
      " [  8.47847848]\n",
      " [  8.4984985 ]\n",
      " [  8.51851852]\n",
      " [  8.53853854]\n",
      " [  8.55855856]\n",
      " [  8.57857858]\n",
      " [  8.5985986 ]\n",
      " [  8.61861862]\n",
      " [  8.63863864]\n",
      " [  8.65865866]\n",
      " [  8.67867868]\n",
      " [  8.6986987 ]\n",
      " [  8.71871872]\n",
      " [  8.73873874]\n",
      " [  8.75875876]\n",
      " [  8.77877878]\n",
      " [  8.7987988 ]\n",
      " [  8.81881882]\n",
      " [  8.83883884]\n",
      " [  8.85885886]\n",
      " [  8.87887888]\n",
      " [  8.8988989 ]\n",
      " [  8.91891892]\n",
      " [  8.93893894]\n",
      " [  8.95895896]\n",
      " [  8.97897898]\n",
      " [  8.998999  ]\n",
      " [  9.01901902]\n",
      " [  9.03903904]\n",
      " [  9.05905906]\n",
      " [  9.07907908]\n",
      " [  9.0990991 ]\n",
      " [  9.11911912]\n",
      " [  9.13913914]\n",
      " [  9.15915916]\n",
      " [  9.17917918]\n",
      " [  9.1991992 ]\n",
      " [  9.21921922]\n",
      " [  9.23923924]\n",
      " [  9.25925926]\n",
      " [  9.27927928]\n",
      " [  9.2992993 ]\n",
      " [  9.31931932]\n",
      " [  9.33933934]\n",
      " [  9.35935936]\n",
      " [  9.37937938]\n",
      " [  9.3993994 ]\n",
      " [  9.41941942]\n",
      " [  9.43943944]\n",
      " [  9.45945946]\n",
      " [  9.47947948]\n",
      " [  9.4994995 ]\n",
      " [  9.51951952]\n",
      " [  9.53953954]\n",
      " [  9.55955956]\n",
      " [  9.57957958]\n",
      " [  9.5995996 ]\n",
      " [  9.61961962]\n",
      " [  9.63963964]\n",
      " [  9.65965966]\n",
      " [  9.67967968]\n",
      " [  9.6996997 ]\n",
      " [  9.71971972]\n",
      " [  9.73973974]\n",
      " [  9.75975976]\n",
      " [  9.77977978]\n",
      " [  9.7997998 ]\n",
      " [  9.81981982]\n",
      " [  9.83983984]\n",
      " [  9.85985986]\n",
      " [  9.87987988]\n",
      " [  9.8998999 ]\n",
      " [  9.91991992]\n",
      " [  9.93993994]\n",
      " [  9.95995996]\n",
      " [  9.97997998]\n",
      " [ 10.        ]]\n"
     ]
    }
   ],
   "source": [
    "x = np.linspace(-10, 10, 1000)\n",
    "print(x)\n",
    "x = x.reshape(len(x), 1)\n",
    "print(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}